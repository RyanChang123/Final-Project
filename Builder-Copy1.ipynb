{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b29c52-978d-4b5d-bbea-0142d29e7a75",
   "metadata": {},
   "source": [
    "# Slow Grower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad473154-3532-4927-9341-0d372aaee9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AAWW: 'Net Income'\n",
      "Error fetching data for AAXN: 'Net Income'\n",
      "Error fetching data for ABC: 'Net Income'\n",
      "Error fetching data for ABTX: 'Net Income'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of Ticker and last 3 Year NI\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_net_income(tickers):\n",
    "    net_income_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch the financials data for the ticker\n",
    "            fin_data = yf.Ticker(ticker).financials\n",
    "            # Get the net income for the last 20 years\n",
    "            net_income = fin_data.loc['Net Income'].iloc[-20:]\n",
    "            # Extract the year from the index and group by year\n",
    "            net_income.index = pd.to_datetime(net_income.index)\n",
    "            net_income = net_income.groupby(net_income.index.year).sum()\n",
    "            net_income_data[ticker] = net_income\n",
    "        except Exception as e:\n",
    "            # If there's an error fetching data, skip the ticker and print the error\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    return net_income_data\n",
    "\n",
    "# Load tickers from the CSV file\n",
    "wilshire_df = pd.read_csv('Wilkshire_5000.csv')\n",
    "tickers = wilshire_df['Ticker'].tolist()\n",
    "\n",
    "# Get net income data for all tickers\n",
    "net_income_data = get_net_income(tickers)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(net_income_data)\n",
    "\n",
    "# Transpose the DataFrame to have tickers as columns\n",
    "df = df.T\n",
    "\n",
    "# Rename columns with year_NI format\n",
    "df.columns = [f\"{year}_NI\" for year in df.columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('net_income_data_grouped.csv')\n",
    "\n",
    "print(\"Net Income Data saved to 'net_income_data_grouped.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b1be5b-5418-4f8a-89d3-0da839fd2d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAWW: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching dividend data for AAWW: 'RangeIndex' object has no attribute 'year'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m tickers \u001b[38;5;241m=\u001b[39m wilshire_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get dividend data for all tickers\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m dividend_data \u001b[38;5;241m=\u001b[39m get_dividends(tickers)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dividend_data)\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mget_dividends\u001b[0;34m(tickers)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Fetch dividend data for the ticker\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         div_data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(ticker)\u001b[38;5;241m.\u001b[39mdividends\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Filter dividends for the years 2020-2023\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         div_data_filtered \u001b[38;5;241m=\u001b[39m div_data\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/ticker.py:134\u001b[0m, in \u001b[0;36mTicker.dividends\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdividends\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dividends()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:1956\u001b[0m, in \u001b[0;36mTickerBase.get_dividends\u001b[0;34m(self, proxy)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dividends\u001b[39m(\u001b[38;5;28mself\u001b[39m, proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1956\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, proxy\u001b[38;5;241m=\u001b[39mproxy)\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividends\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history:\n\u001b[1;32m   1958\u001b[0m         dividends \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividends\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/utils.py:103\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:141\u001b[0m, in \u001b[0;36mTickerBase.history\u001b[0;34m(self, period, interval, start, end, prepost, actions, auto_adjust, back_adjust, repair, keepna, proxy, rounding, timeout, raise_errors)\u001b[0m\n\u001b[1;32m    138\u001b[0m end_user \u001b[38;5;241m=\u001b[39m end\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mor\u001b[39;00m period \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m period\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Check can get TZ. Fail => probably delisted\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticker_tz(proxy, timeout)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# Every valid ticker has a timezone. Missing = problem\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo timezone found, symbol may be delisted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:1661\u001b[0m, in \u001b[0;36mTickerBase._get_ticker_tz\u001b[0;34m(self, proxy, timeout)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1661\u001b[0m     tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_ticker_tz(proxy, timeout)\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_valid_timezone(tz):\n\u001b[1;32m   1664\u001b[0m         \u001b[38;5;66;03m# info fetch is relatively slow so cache timezone\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m         c\u001b[38;5;241m.\u001b[39mstore(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticker, tz)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/utils.py:103\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:1684\u001b[0m, in \u001b[0;36mTickerBase._fetch_ticker_tz\u001b[0;34m(self, proxy, timeout)\u001b[0m\n\u001b[1;32m   1681\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/v8/finance/chart/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1684\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcache_get(url\u001b[38;5;241m=\u001b[39murl, params\u001b[38;5;241m=\u001b[39mparams, proxy\u001b[38;5;241m=\u001b[39mproxy, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1685\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/data.py:28\u001b[0m, in \u001b[0;36mlru_cache_freezeargs.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(arg) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     27\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mtuple\u001b[39m(v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/data.py:387\u001b[0m, in \u001b[0;36mYfData.cache_get\u001b[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;129m@lru_cache_freezeargs\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39mcache_maxsize)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, user_agent_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(url, user_agent_headers, params, proxy, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/utils.py:103\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/data.py:379\u001b[0m, in \u001b[0;36mYfData.get\u001b[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    378\u001b[0m         request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcookies\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {cookie\u001b[38;5;241m.\u001b[39mname: cookie\u001b[38;5;241m.\u001b[39mvalue}\n\u001b[0;32m--> 379\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_args)\n\u001b[1;32m    380\u001b[0m     utils\u001b[38;5;241m.\u001b[39mget_yf_logger()\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse code=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of Ticker and last 3 Year DIV\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_dividends(tickers):\n",
    "    dividend_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch dividend data for the ticker\n",
    "            div_data = yf.Ticker(ticker).dividends\n",
    "            # Filter dividends for the years 2020-2023\n",
    "            div_data_filtered = div_data.loc['2020':'2023']\n",
    "            # Calculate the total amount of dividends paid for each year\n",
    "            total_dividends = div_data_filtered.groupby(div_data_filtered.index.year).sum()\n",
    "            dividend_data[ticker] = total_dividends\n",
    "        except Exception as e:\n",
    "            # If there's an error fetching data, skip the ticker and print the error\n",
    "            print(f\"Error fetching dividend data for {ticker}: {e}\")\n",
    "    return dividend_data\n",
    "\n",
    "# Load tickers from the CSV file\n",
    "wilshire_df = pd.read_csv('Wilkshire_5000.csv')\n",
    "tickers = wilshire_df['Ticker'].tolist()\n",
    "\n",
    "# Get dividend data for all tickers\n",
    "dividend_data = get_dividends(tickers)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(dividend_data)\n",
    "\n",
    "# Transpose the DataFrame to have years as columns\n",
    "df = df.T\n",
    "\n",
    "# Rename columns with year_div format\n",
    "df.columns = [f\"{year}_Div\" for year in df.columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('dividend_data_grouped.csv')\n",
    "\n",
    "print(\"Dividend Data saved to 'dividend_data_grouped.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c79225e-ed4c-4982-b7b0-fba647b1cb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data saved to 'merged_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# List of Ticker and merged NI and DIV last 4 years and lists dividend growth \n",
    "import pandas as pd\n",
    "\n",
    "# Load net income data\n",
    "net_income_df = pd.read_csv('net_income_data_grouped.csv', index_col=0)\n",
    "\n",
    "# Load dividend data\n",
    "dividend_df = pd.read_csv('dividend_data_grouped.csv', index_col=0)\n",
    "\n",
    "# Merge dataframes on index (tickers)\n",
    "merged_df = pd.merge(net_income_df, dividend_df, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate dividend growth rates for consecutive years\n",
    "for year in range(2020, 2023):\n",
    "    current_col = f\"{year}_Div\"\n",
    "    next_col = f\"{year+1}_Div\"\n",
    "    growth_col = f\"{year}_{year+1}_Div_Grow\"\n",
    "    merged_df[growth_col] = (merged_df[next_col] - merged_df[current_col]) / merged_df[current_col]\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_data.csv')\n",
    "\n",
    "print(\"Merged Data saved to 'merged_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57da1c7-c394-4657-bc79-d602a547a8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# Rename the first column to 'Ticker'\n",
    "df = df.rename(columns={'Unnamed: 0': 'Ticker'})\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('Slow_Grow_Master.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ba16c-2497-4b6c-80a2-4b6fabb218ce",
   "metadata": {},
   "source": [
    "# Stalwart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07e4173-ec23-4b7d-bf70-8de39416f86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to modified_net_income_data.csv\n"
     ]
    }
   ],
   "source": [
    "# adds the net income growth columns \n",
    "import pandas as pd\n",
    "\n",
    "# Read data from CSV file\n",
    "net_income_df = pd.read_csv(\"net_income_data_grouped.csv\")\n",
    "\n",
    "# Drop the '2024_NI' column\n",
    "net_income_df.drop(columns=['2024_NI'], inplace=True)\n",
    "\n",
    "# Calculate net income growth as a percentage for each specified year\n",
    "net_income_df['2020_2021_NI_Grow'] = ((net_income_df['2021_NI'] - net_income_df['2020_NI']) / net_income_df['2020_NI']) * 100\n",
    "net_income_df['2021_2022_NI_Grow'] = ((net_income_df['2022_NI'] - net_income_df['2021_NI']) / net_income_df['2021_NI']) * 100\n",
    "net_income_df['2022_2023_NI_Grow'] = ((net_income_df['2023_NI'] - net_income_df['2022_NI']) / net_income_df['2022_NI']) * 100\n",
    "\n",
    "# Round the percentage growth to 4 decimal places\n",
    "net_income_df = net_income_df.round({'2020_2021_NI_Grow': 4, '2021_2022_NI_Grow': 4, '2022_2023_NI_Grow': 4})\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "net_income_df.to_csv(\"modified_net_income_data.csv\", index=False)\n",
    "\n",
    "print(\"Modified data saved to modified_net_income_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9074beee-bfaa-4245-a997-481e35827239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/CORR?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=CORR&crumb=1OeHav735G1\n",
      "404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/TMST?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=TMST&crumb=1OeHav735G1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ticker_2023_PE.csv' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Creates a csv file for PE ratio\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get forward PE ratio from Yahoo Finance\n",
    "def get_forward_pe(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get forward PE ratio\n",
    "        forward_pe = ticker_data.info['forwardPE']\n",
    "        return forward_pe\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_PE'] = data['Ticker'].apply(get_forward_pe)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_PE' columns\n",
    "result_df = data[['Ticker', '2023_PE']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_PE.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_PE.csv' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69d97d-6d8f-40e3-ad8b-8ed94d4608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a csv file for GSector \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get GICS sector from Yahoo Finance\n",
    "def get_gsector(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get GICS sector\n",
    "        gsector = ticker_data.info['sector']\n",
    "        return gsector\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['Gsector'] = data['Ticker'].apply(get_gsector)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and 'Gsector' columns\n",
    "result_df = data[['Ticker', 'Gsector']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_Gsector.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_Gsector.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5be66c-fb2b-419e-aa02-ed7df529f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a csv file for Market Cap\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get market capitalization from Yahoo Finance\n",
    "def get_market_cap(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get market cap\n",
    "        market_cap = ticker_data.info['marketCap']\n",
    "        return market_cap\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_MarketCap'] = data['Ticker'].apply(get_market_cap)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_MarketCap' columns\n",
    "result_df = data[['Ticker', '2023_MarketCap']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_MarketCap.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_MarketCap.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7ffb3-8ddf-4d00-9819-c1153bd428f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Merged CSV between ticker PE and Gsector\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from CSV files\n",
    "ticker_2023_PE_df = pd.read_csv(\"ticker_2023_PE.csv\")\n",
    "ticker_Gsector_df = pd.read_csv(\"ticker_Gsector.csv\")\n",
    "\n",
    "# Merge the two DataFrames based on the 'Ticker' column\n",
    "merged_df = pd.merge(ticker_2023_PE_df, ticker_Gsector_df, on=\"Ticker\")\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file labeled as \"PE_Gsector.csv\"\n",
    "merged_df.to_csv(\"PE_Gsector.csv\", index=False)\n",
    "\n",
    "print(\"Merged data saved to PE_Gsector.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642142d3-bcec-4d77-999d-d61d9f5c9798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PE_Gsector.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read data from the merged CSV file and drop rows with missing values\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPE_Gsector.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate average PE ratio by Gsector\u001b[39;00m\n\u001b[1;32m      8\u001b[0m avg_pe_by_sector \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGsector\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023_PE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PE_Gsector.csv'"
     ]
    }
   ],
   "source": [
    "# creates a merged dataframe of PE and Gsector \n",
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file and drop rows with missing values\n",
    "merged_df = pd.read_csv(\"PE_Gsector.csv\").dropna()\n",
    "\n",
    "# Calculate average PE ratio by Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "avg_pe_by_sector.rename(columns={'2023_PE': 'Avg_PE_Gsector'}, inplace=True)\n",
    "\n",
    "# Merge the average PE ratios back into the original DataFrame based on Gsector\n",
    "merged_df = pd.merge(merged_df, avg_pe_by_sector, on=\"Gsector\")\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"PE_Gsector_with_avg.csv\", index=False)\n",
    "\n",
    "print(\"Modified data saved to PE_Gsector_with_avg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab45a8b-4c18-46e4-81e3-976fc1796133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a data frame that calcuates the average PE ratio by Gsector \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with comma as delimiter\n",
    "df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Remove rows with \"inf\" values in 2023_PE column\n",
    "df = df[df['2023_PE'] != float('inf')]\n",
    "\n",
    "# Calculate average PE ratio for each sector\n",
    "avg_pe_by_sector = df.groupby('Gsector')['2023_PE'].mean()\n",
    "\n",
    "# Fill in the Avg_PE_Gsector column based on the calculated averages\n",
    "df['Avg_PE_Gsector'] = df['Gsector'].map(avg_pe_by_sector)\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"cleaned_PE_Gsector_with_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ba829-e6e5-49d6-a2ad-725926416178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame that labels based on market cap size \n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column and create a new column for the category\n",
    "data['Market_Cap_Cat'] = data['2023_MarketCap'].apply(categorize_market_cap)\n",
    "\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "data.to_csv(\"ticker_2023_MarketCap_with_Category.csv\", index=False)\n",
    "\n",
    "print(\"New CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85dc67-b24f-4431-93d7-bae306c69c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines the \"ticker_2023_MarketCap_with_Category.csv\" , \"cleaned_PE_Gsector_with_avg.csv\" and \"\"modified_net_income_data.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "market_cap_data = pd.read_csv(\"ticker_2023_MarketCap_with_Category.csv\")\n",
    "pe_gsector_data = pd.read_csv(\"cleaned_PE_Gsector_with_avg.csv\")\n",
    "net_income_data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Rename the first column in net_income_data to \"Ticker\"\n",
    "net_income_data.rename(columns={net_income_data.columns[0]: 'Ticker'}, inplace=True)\n",
    "\n",
    "# Merge the CSV files based on the 'Ticker' column\n",
    "merged_data = pd.merge(market_cap_data, pe_gsector_data, on='Ticker', how='outer')\n",
    "merged_data = pd.merge(merged_data, net_income_data, on='Ticker', how='outer')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Stalwart_Master.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc1b95-9cb5-4776-866d-a2d783254b1d",
   "metadata": {},
   "source": [
    "# Fast grower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e58640-8fcc-4bf3-b28d-b3984ae0d08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates a csv that combines the Net income growth and PE ratios \n",
    "import pandas as pd\n",
    "\n",
    "# Read the modified_net_income_data.csv\n",
    "net_income_df = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Rename the first column to 'Ticker'\n",
    "net_income_df.rename(columns={net_income_df.columns[0]: 'Ticker'}, inplace=True)\n",
    "\n",
    "# Remove columns 2020_NI, 2021_NI, 2022_NI, 2023_NI\n",
    "net_income_df.drop(columns=['2020_NI', '2021_NI', '2022_NI', '2023_NI'], inplace=True)\n",
    "\n",
    "# Read the ticker_2023_PE.csv\n",
    "pe_df = pd.read_csv(\"ticker_2023_PE.csv\")\n",
    "\n",
    "# Merge the two dataframes on the 'Ticker' column\n",
    "merged_df = pd.merge(net_income_df, pe_df, on='Ticker')\n",
    "\n",
    "# Rename the 2023_PE column\n",
    "merged_df.rename(columns={'PE': '2023_PE'}, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(\"NI_Grow_PE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca16f95-b863-4d45-b05c-5703ef1601af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reades the created file and categoriezes based on ideal growth rate \n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"NI_Grow_PE.csv\")\n",
    "\n",
    "# Calculate average growth rate for each row\n",
    "df['Average_Growth'] = df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Categorize the growth rate\n",
    "def categorize_growth(average_growth):\n",
    "    if 20 <= average_growth <= 25:\n",
    "        return 'Ideal 20% - 25%'\n",
    "    else:\n",
    "        return 'Not Ideal'\n",
    "\n",
    "df['Growth_Rate'] = df['Average_Growth'].apply(categorize_growth)\n",
    "\n",
    "# Save the updated CSV with the name \"Fast_Grower_Master.csv\"\n",
    "df.to_csv(\"Fast_Grower_Master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9856c-c379-4858-b830-6754872addf8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3637ced3-a895-45af-8d0c-3f1c0cf60892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drops all missing values from the master csvs\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Slow_Grow_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Slow_Grow_Master.csv\", index=False)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Fast_Grower_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Fast_Grower_Master.csv\", index=False)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Stalwart_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Stalwart_Master.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8333e-05f4-4ebe-bb28-7c99a6cb1fe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Screen Slow Growers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721a3991-3a43-4b4c-ad35-e049d0c08ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Screens based on dividend growth \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Slow_Grow_Master.csv\")\n",
    "\n",
    "# Filter companies with positive or flat growing dividends for each year\n",
    "filtered_df = df[(df['2020_2021_Div_Grow'] >= 0) & \n",
    "                 (df['2021_2022_Div_Grow'] >= 0) & \n",
    "                 (df['2022_2023_Div_Grow'] >= 0)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Slow_Grow.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b5e99-ba5c-47ae-82ca-6c32f4746e9e",
   "metadata": {},
   "source": [
    "# Screen Stalwarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab947-bfa8-407b-9b54-38ab8bbee515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Stalwart_Master.csv\")\n",
    "\n",
    "# Filter companies with Market_Cap_Cat as 'mega cap' or 'large cap'\n",
    "filtered_df = df[df['Market_Cap_Cat'].isin(['mega-cap', 'large-cap'])]\n",
    "\n",
    "# Calculate average Net Income growth across the three years\n",
    "df['Avg_NI_Growth'] = df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Filter companies where average Net Income growth is between + or - 3% from the industry PE ratio\n",
    "filtered_df = filtered_df[(df['Avg_NI_Growth'] >= 0.97 * df['Avg_PE_Gsector']) & \n",
    "                          (df['Avg_NI_Growth'] <= 1.03 * df['Avg_PE_Gsector'])]\n",
    "\n",
    "# Add a new column for average NI growth\n",
    "filtered_df['Avg_NI_Growth'] = filtered_df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Stalwart.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76ea9-f9a8-417a-ac3e-54e10d18f445",
   "metadata": {},
   "source": [
    "# Screen Fast Growers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7eb8a6-bf2e-4e4a-9e35-3cb914b62db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Fast_Grower_Master.csv\", sep=\"\\t\")\n",
    "\n",
    "# Split the concatenated column names into individual names\n",
    "column_names = df.columns[0].split(',')\n",
    "\n",
    "# Create a new DataFrame with the split values\n",
    "new_df = df.iloc[:, 0].str.split(',', expand=True)\n",
    "\n",
    "# Assign the split column names to the new DataFrame\n",
    "new_df.columns = column_names\n",
    "\n",
    "# Convert 'Average_Growth' and '2023_PE' columns to numeric\n",
    "new_df['Average_Growth'] = pd.to_numeric(new_df['Average_Growth'], errors='coerce')\n",
    "new_df['2023_PE'] = pd.to_numeric(new_df['2023_PE'], errors='coerce')\n",
    "\n",
    "# Filter rows where the average growth rate is between 20% and 25% and within +3 or -3 from the PE ratio\n",
    "filtered_df = new_df[(new_df['Average_Growth'] >= 20) & \n",
    "                     (new_df['Average_Growth'] <= 25) &\n",
    "                     (new_df['Average_Growth'] >= new_df['2023_PE'] - 3) & \n",
    "                     (new_df['Average_Growth'] <= new_df['2023_PE'] + 3)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Fast_Grower.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13992658-776a-471c-979a-48c210abdbe5",
   "metadata": {},
   "source": [
    "# Comparative Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a731e-e45a-4a2d-9e38-13b9661d926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_sp500_data():\n",
    "    # Define the ticker symbol for S&P 500\n",
    "    ticker_symbol = \"^GSPC\"\n",
    "    \n",
    "    # Define the start and end dates\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    \n",
    "    # Fetch the historical data\n",
    "    sp500_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "    \n",
    "    return sp500_data\n",
    "\n",
    "def get_annual_closing_prices(data):\n",
    "    # Resample the data to annual frequency and extract closing prices\n",
    "    annual_closing_prices = data['Close'].resample('Y').last()\n",
    "    \n",
    "    return annual_closing_prices\n",
    "\n",
    "# Fetch S&P 500 data\n",
    "sp500_data = get_sp500_data()\n",
    "\n",
    "# Get annual closing prices\n",
    "annual_closing_prices = get_annual_closing_prices(sp500_data)\n",
    "\n",
    "# Print the annual closing prices\n",
    "print(annual_closing_prices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e257bf0-09a2-4b73-9fe6-49c591d84e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2020-12-31    39456.660156\n",
      "2021-12-31    48461.160156\n",
      "2022-12-31    38073.941406\n",
      "2023-12-31    48295.378906\n",
      "Freq: A-DEC, Name: Close, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports the Wilkshire closing values from 2020 - 2023\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_wilshire5000_data():\n",
    "    # Define the ticker symbol for Wilshire 5000\n",
    "    ticker_symbol = \"^W5000\"\n",
    "    \n",
    "    # Define the start and end dates\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    \n",
    "    # Fetch the historical data\n",
    "    wilshire5000_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "    \n",
    "    return wilshire5000_data\n",
    "\n",
    "def get_annual_closing_prices(data):\n",
    "    # Resample the data to annual frequency and extract closing prices\n",
    "    annual_closing_prices = data['Close'].resample('Y').last()\n",
    "    \n",
    "    return annual_closing_prices\n",
    "\n",
    "# Fetch Wilshire 5000 data\n",
    "wilshire5000_data = get_wilshire5000_data()\n",
    "\n",
    "# Get annual closing prices\n",
    "annual_closing_prices_wilshire = get_annual_closing_prices(wilshire5000_data)\n",
    "\n",
    "# Print the annual closing prices\n",
    "print(annual_closing_prices_wilshire)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15348027-70d2-48d3-ac28-48109488de86",
   "metadata": {},
   "source": [
    "# Get closing prices from screened companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbca8ed-e862-47a9-8fcc-5f909a72a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow Grower Closing Prices and market cap\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Slow_Grow_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Slow_Grow.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8df215-0655-4d0f-b03c-e8d538006737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow Grower market weighted returns \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Slow_Grow_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ef287-7dcb-4c2d-ac84-b298f6d5c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stalwart Closing Prices and Market Cap \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Stalwart_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Stalwart.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6694c-59bf-4251-a2a0-5309af7a0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stalwart Market weighted return \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Stalwart_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17519218-f468-4391-931e-9490e7698862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Grower Closing Prices and Market Cap \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Fast_Grower_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Fast_Grower.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7213c-aa57-4f27-9210-b6723e69c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Grower Market Weighted Return \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Fast_Grower_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02558653-8b68-4616-9b56-45162983eb73",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32a0b3-8078-4dd3-8c29-11a95fbc2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data for each index\n",
    "# Data from \n",
    "data = {\n",
    "    'Slow Grower': {2020: 163.438749, 2021: 222.309990, 2022: 185.809803, 2023: 266.864258},\n",
    "    'Stalwart': {2020: 332.570957, 2021: 463.625380, 2022: 390.856517, 2023: 564.568406},\n",
    "    'Fast Grower': {2020: 101.064051, 2021: 143.920488, 2022: 134.847909, 2023: 150.171399},\n",
    "    'S&P 500': {2020: 3756.070068359375, 2021: 4766.180176, 2022: 3839.5, 2023: 4769.830078},\n",
    "    'Wilshire 5000': {2020: 39456.66016, 2021: 48461.16016, 2022: 38073.94141, 2023: 48295.37891}\n",
    "}\n",
    "\n",
    "# Function to calculate CAGR\n",
    "def calculate_cagr(start_value, end_value, periods):\n",
    "    return (end_value / start_value) ** (1 / periods) - 1\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize index values by dividing each value by the starting value (2020 value)\n",
    "normalized_df = df.divide(df.iloc[0])\n",
    "\n",
    "# Calculate CAGR for each series and add to the plot\n",
    "cagr_values = {}\n",
    "for column in normalized_df.columns:\n",
    "    cagr = calculate_cagr(normalized_df[column].iloc[0], normalized_df[column].iloc[-1], len(normalized_df[column]) - 1)\n",
    "    cagr_values[column] = f\"{cagr:.2%}\"\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for column in normalized_df.columns:\n",
    "    ax.plot(normalized_df.index, normalized_df[column], marker='o', label=f\"{column} (CAGR: {cagr_values[column]})\")\n",
    "\n",
    "ax.set_title('Normalized Index Performance Comparison 2020-2023')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Normalized Index Value')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
