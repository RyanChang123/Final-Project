{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b29c52-978d-4b5d-bbea-0142d29e7a75",
   "metadata": {},
   "source": [
    "# Slow Grower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad473154-3532-4927-9341-0d372aaee9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'Net Income' data available for AAWW\n",
      "No 'Net Income' data available for AAXN\n",
      "No 'Net Income' data available for ABC\n",
      "No 'Net Income' data available for ABTX\n",
      "No 'Net Income' data available for ACBI\n",
      "No 'Net Income' data available for ACC\n",
      "No 'Net Income' data available for ACER\n",
      "No 'Net Income' data available for ACIA\n",
      "No 'Net Income' data available for ACRX\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m tickers \u001b[38;5;241m=\u001b[39m wilshire_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get net income data for all tickers\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m net_income_data \u001b[38;5;241m=\u001b[39m get_net_income(tickers)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n\u001b[1;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(net_income_data)\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mget_net_income\u001b[0;34m(tickers)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Fetch the financials data for the ticker\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         fin_data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(ticker)\u001b[38;5;241m.\u001b[39mfinancials\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fin_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNet Income\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fin_data\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;66;03m# Get the net income for the last 20 years\u001b[39;00m\n\u001b[1;32m     13\u001b[0m             net_income \u001b[38;5;241m=\u001b[39m fin_data\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNet Income\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/ticker.py:205\u001b[0m, in \u001b[0;36mTicker.financials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinancials\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincome_stmt\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/ticker.py:189\u001b[0m, in \u001b[0;36mTicker.income_stmt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mincome_stmt\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_income_stmt(pretty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:1879\u001b[0m, in \u001b[0;36mTickerBase.get_income_stmt\u001b[0;34m(self, proxy, as_dict, pretty, freq)\u001b[0m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;124;03m:Parameters:\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;124;03m    as_dict: bool\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;124;03m        Default is None\u001b[39;00m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fundamentals\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;241m=\u001b[39m proxy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\n\u001b[0;32m-> 1879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fundamentals\u001b[38;5;241m.\u001b[39mfinancials\u001b[38;5;241m.\u001b[39mget_income_time_series(freq\u001b[38;5;241m=\u001b[39mfreq, proxy\u001b[38;5;241m=\u001b[39mproxy)\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretty:\n\u001b[1;32m   1882\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/scrapers/fundamentals.py:55\u001b[0m, in \u001b[0;36mFinancials.get_income_time_series\u001b[0;34m(self, freq, proxy)\u001b[0m\n\u001b[1;32m     53\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_income_time_series\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m---> 55\u001b[0m     res[freq] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_time_series(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq, proxy)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res[freq]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/utils.py:103\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/scrapers/fundamentals.py:85\u001b[0m, in \u001b[0;36mFinancials._fetch_time_series\u001b[0;34m(self, name, timescale, proxy)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal argument: timescale must be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_timescales\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     statement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_financials_table(name, timescale, proxy)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m statement \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m statement\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/scrapers/fundamentals.py:101\u001b[0m, in \u001b[0;36mFinancials._create_financials_table\u001b[0;34m(self, name, timescale, proxy)\u001b[0m\n\u001b[1;32m     98\u001b[0m keys \u001b[38;5;241m=\u001b[39m const\u001b[38;5;241m.\u001b[39mfundamentals_keys[name]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_financials_time_series(timescale, keys, proxy)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/scrapers/fundamentals.py:141\u001b[0m, in \u001b[0;36mFinancials.get_financials_time_series\u001b[0;34m(self, timescale, keys, proxy)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mdates, index\u001b[38;5;241m=\u001b[39m[k])\n\u001b[0;32m--> 141\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[k] \u001b[38;5;241m=\u001b[39m {pd\u001b[38;5;241m.\u001b[39mTimestamp(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masOfDate\u001b[39m\u001b[38;5;124m\"\u001b[39m]): x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportedValue\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m v}\n\u001b[1;32m    143\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m timescale, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Reorder table to match order on Yahoo website\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1837\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1837\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:2068\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2062\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m maybe_convert_ix(\u001b[38;5;241m*\u001b[39mindexer)  \u001b[38;5;66;03m# e.g. test_setitem_frame_align\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(value, ABCSeries) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;66;03m# TODO(EA): ExtensionBlock.setitem this causes issues with\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;66;03m# setting for extensionarrays that store dicts. Need to decide\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;66;03m# if it's worth supporting that.\u001b[39;00m\n\u001b[0;32m-> 2068\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_series(indexer, Series(value))\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2071\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_frame(indexer, value)\u001b[38;5;241m.\u001b[39m_values\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:472\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    470\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(data):\n\u001b[0;32m--> 472\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_dict(data, index, dtype)\n\u001b[1;32m    473\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:565\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    562\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;241m0\u001b[39m), []\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Input is now list-like, so rely on \"standard\" construction:\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m s \u001b[38;5;241m=\u001b[39m Series(values, index\u001b[38;5;241m=\u001b[39mkeys, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:425\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    422\u001b[0m name \u001b[38;5;241m=\u001b[39m ibase\u001b[38;5;241m.\u001b[39mmaybe_extract_name(name, data, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dtype(dtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:7128\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:558\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex data must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m arr \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(arr)\n\u001b[1;32m    560\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype_to_subclass(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    562\u001b[0m arr \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m_ensure_array(arr, arr\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/construction.py:466\u001b[0m, in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeArray\u001b[38;5;241m.\u001b[39m_from_sequence(arr)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimedeltaArray\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:291\u001b[0m, in \u001b[0;36mDatetimeArray._from_sequence\u001b[0;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_sequence\u001b[39m(\u001b[38;5;28mcls\u001b[39m, scalars, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_sequence_not_strict(scalars, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:333\u001b[0m, in \u001b[0;36mDatetimeArray._from_sequence_not_strict\u001b[0;34m(cls, data, dtype, copy, tz, freq, dayfirst, yearfirst, ambiguous)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;66;03m# DatetimeTZDtype\u001b[39;00m\n\u001b[1;32m    331\u001b[0m         unit \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39munit\n\u001b[0;32m--> 333\u001b[0m subarr, tz, inferred_freq \u001b[38;5;241m=\u001b[39m _sequence_to_dt64ns(\n\u001b[1;32m    334\u001b[0m     data,\n\u001b[1;32m    335\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    336\u001b[0m     tz\u001b[38;5;241m=\u001b[39mtz,\n\u001b[1;32m    337\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m    338\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m    339\u001b[0m     ambiguous\u001b[38;5;241m=\u001b[39mambiguous,\n\u001b[1;32m    340\u001b[0m     out_unit\u001b[38;5;241m=\u001b[39munit,\n\u001b[1;32m    341\u001b[0m )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# We have to call this again after possibly inferring a tz above\u001b[39;00m\n\u001b[1;32m    343\u001b[0m _validate_tz_from_dtype(dtype, tz, explicit_tz_none)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:2032\u001b[0m, in \u001b[0;36m_sequence_to_dt64ns\u001b[0;34m(data, copy, tz, dayfirst, yearfirst, ambiguous, out_unit)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     inferred_freq \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfreq\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;66;03m# By this point we are assured to have either a numpy array or Index\u001b[39;00m\n\u001b[0;32m-> 2032\u001b[0m data, copy \u001b[38;5;241m=\u001b[39m maybe_convert_dtype(data, copy, tz\u001b[38;5;241m=\u001b[39mtz)\n\u001b[1;32m   2033\u001b[0m data_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2035\u001b[0m out_dtype \u001b[38;5;241m=\u001b[39m DT64NS_DTYPE\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:2233\u001b[0m, in \u001b[0;36mmaybe_convert_dtype\u001b[0;34m(data, copy, tz)\u001b[0m\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m# e.g. collections.deque\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data, copy\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_float_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# pre-2.0 we treated these as wall-times, inconsistent with ints\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# GH#23675, GH#45573 deprecated to treat symmetrically with integer dtypes.\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     \u001b[38;5;66;03m# Note: data.astype(np.int64) fails ARM tests, see\u001b[39;00m\n\u001b[1;32m   2237\u001b[0m     \u001b[38;5;66;03m# https://github.com/pandas-dev/pandas/issues/49468.\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mastype(DT64NS_DTYPE)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2239\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/common.py:1239\u001b[0m, in \u001b[0;36mis_float_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_float_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m    Check whether the provided array or dtype is of a float dtype.\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_dtype_type(arr_or_dtype, classes(np\u001b[38;5;241m.\u001b[39mfloating)) \u001b[38;5;129;01mor\u001b[39;00m _is_dtype(\n\u001b[1;32m   1240\u001b[0m         arr_or_dtype, \u001b[38;5;28;01mlambda\u001b[39;00m typ: \u001b[38;5;28misinstance\u001b[39m(typ, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m typ\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/common.py:1435\u001b[0m, in \u001b[0;36m_is_dtype\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m    Check whether the provided array or dtype is of a complex dtype.\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_dtype_type(arr_or_dtype, classes(np\u001b[38;5;241m.\u001b[39mcomplexfloating))\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_dtype\u001b[39m(arr_or_dtype, condition) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;124;03m    Return true if the condition is satisfied for the arr_or_dtype.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr_or_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_net_income(tickers):\n",
    "    net_income_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch the financials data for the ticker\n",
    "            fin_data = yf.Ticker(ticker).financials\n",
    "            if fin_data is not None and 'Net Income' in fin_data.index:\n",
    "                # Get the net income for the last 20 years\n",
    "                net_income = fin_data.loc['Net Income'].iloc[-20:]\n",
    "                # Extract the year from the index and group by year\n",
    "                net_income.index = pd.to_datetime(net_income.index)\n",
    "                net_income = net_income.groupby(net_income.index.year).sum()\n",
    "                net_income_data[ticker] = net_income\n",
    "            else:\n",
    "                print(f\"No 'Net Income' data available for {ticker}\")\n",
    "        except Exception as e:\n",
    "            # If there's an error fetching data, skip the ticker and print the error\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    return net_income_data\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'Inputs/net_income_data_grouped.csv'\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"Data file '{output_file_path}' already exists. Loading data from file.\")\n",
    "    df = pd.read_csv(output_file_path, index_col=0)\n",
    "else:\n",
    "    # Load tickers from the CSV file\n",
    "    wilshire_df = pd.read_csv('Wilkshire_5000.csv')\n",
    "    tickers = wilshire_df['Ticker'].tolist()\n",
    "\n",
    "    # Get net income data for all tickers\n",
    "    net_income_data = get_net_income(tickers)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(net_income_data)\n",
    "\n",
    "    # Transpose the DataFrame to have tickers as columns\n",
    "    df = df.T\n",
    "\n",
    "    # Rename columns with year_NI format\n",
    "    df.columns = [f\"{year}_NI\" for year in df.columns]\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the specified path\n",
    "    df.to_csv(output_file_path)\n",
    "\n",
    "    print(f\"Net Income Data saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b1be5b-5418-4f8a-89d3-0da839fd2d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAWW: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching dividend data for AAWW: 'RangeIndex' object has no attribute 'year'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAXN: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching dividend data for AAXN: 'RangeIndex' object has no attribute 'year'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ABC: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching dividend data for ABC: 'RangeIndex' object has no attribute 'year'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m tickers \u001b[38;5;241m=\u001b[39m wilshire_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get dividend data for all tickers\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m dividend_data \u001b[38;5;241m=\u001b[39m get_dividends(tickers)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dividend_data)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mget_dividends\u001b[0;34m(tickers)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Fetch dividend data for the ticker\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         div_data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(ticker)\u001b[38;5;241m.\u001b[39mdividends\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Filter dividends for the years 2020-2023\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         div_data_filtered \u001b[38;5;241m=\u001b[39m div_data\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/ticker.py:134\u001b[0m, in \u001b[0;36mTicker.dividends\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdividends\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dividends()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:1956\u001b[0m, in \u001b[0;36mTickerBase.get_dividends\u001b[0;34m(self, proxy)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dividends\u001b[39m(\u001b[38;5;28mself\u001b[39m, proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1956\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, proxy\u001b[38;5;241m=\u001b[39mproxy)\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividends\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history:\n\u001b[1;32m   1958\u001b[0m         dividends \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividends\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/utils.py:103\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/yfinance/base.py:210\u001b[0m, in \u001b[0;36mTickerBase.history\u001b[0;34m(self, period, interval, start, end, prepost, actions, auto_adjust, back_adjust, repair, keepna, proxy, rounding, timeout, raise_errors)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWill be right back\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01mor\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** YAHOO! FINANCE IS CURRENTLY DOWN! ***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOur engineers are working quickly to resolve \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe issue. Thank you for your patience.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of Ticker and last 3 Year DIV\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_dividends(tickers):\n",
    "    dividend_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch dividend data for the ticker\n",
    "            div_data = yf.Ticker(ticker).dividends\n",
    "            # Filter dividends for the years 2020-2023\n",
    "            div_data_filtered = div_data.loc['2020':'2023']\n",
    "            # Calculate the total amount of dividends paid for each year\n",
    "            total_dividends = div_data_filtered.groupby(div_data_filtered.index.year).sum()\n",
    "            dividend_data[ticker] = total_dividends\n",
    "        except Exception as e:\n",
    "            # If there's an error fetching data, skip the ticker and print the error\n",
    "            print(f\"Error fetching dividend data for {ticker}: {e}\")\n",
    "    return dividend_data\n",
    "\n",
    "# Load tickers from the CSV file\n",
    "wilshire_df = pd.read_csv('Wilkshire_5000.csv')\n",
    "tickers = wilshire_df['Ticker'].tolist()\n",
    "\n",
    "# Get dividend data for all tickers\n",
    "dividend_data = get_dividends(tickers)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(dividend_data)\n",
    "\n",
    "# Transpose the DataFrame to have years as columns\n",
    "df = df.T\n",
    "\n",
    "# Rename columns with year_div format\n",
    "df.columns = [f\"{year}_Div\" for year in df.columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('dividend_data_grouped.csv')\n",
    "\n",
    "print(\"Dividend Data saved to 'dividend_data_grouped.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c79225e-ed4c-4982-b7b0-fba647b1cb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data saved to 'merged_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# List of Ticker and merged NI and DIV last 4 years and lists dividend growth \n",
    "import pandas as pd\n",
    "\n",
    "# Load net income data\n",
    "net_income_df = pd.read_csv('net_income_data_grouped.csv', index_col=0)\n",
    "\n",
    "# Load dividend data\n",
    "dividend_df = pd.read_csv('dividend_data_grouped.csv', index_col=0)\n",
    "\n",
    "# Merge dataframes on index (tickers)\n",
    "merged_df = pd.merge(net_income_df, dividend_df, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate dividend growth rates for consecutive years\n",
    "for year in range(2020, 2023):\n",
    "    current_col = f\"{year}_Div\"\n",
    "    next_col = f\"{year+1}_Div\"\n",
    "    growth_col = f\"{year}_{year+1}_Div_Grow\"\n",
    "    merged_df[growth_col] = (merged_df[next_col] - merged_df[current_col]) / merged_df[current_col]\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_data.csv')\n",
    "\n",
    "print(\"Merged Data saved to 'merged_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ba16c-2497-4b6c-80a2-4b6fabb218ce",
   "metadata": {},
   "source": [
    "# Stalwart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e4173-ec23-4b7d-bf70-8de39416f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from CSV file\n",
    "net_income_df = pd.read_csv(\"net_income_data_grouped.csv\")\n",
    "\n",
    "# Drop the '2024_NI' column\n",
    "net_income_df.drop(columns=['2024_NI'], inplace=True)\n",
    "\n",
    "# Calculate net income growth as a percentage for each specified year\n",
    "net_income_df['2020_2021_NI_Grow'] = ((net_income_df['2021_NI'] - net_income_df['2020_NI']) / net_income_df['2020_NI']) * 100\n",
    "net_income_df['2021_2022_NI_Grow'] = ((net_income_df['2022_NI'] - net_income_df['2021_NI']) / net_income_df['2021_NI']) * 100\n",
    "net_income_df['2022_2023_NI_Grow'] = ((net_income_df['2023_NI'] - net_income_df['2022_NI']) / net_income_df['2022_NI']) * 100\n",
    "\n",
    "# Round the percentage growth to 4 decimal places\n",
    "net_income_df = net_income_df.round({'2020_2021_NI_Grow': 4, '2021_2022_NI_Grow': 4, '2022_2023_NI_Grow': 4})\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file with the first column labeled as 'Ticker'\n",
    "net_income_df.to_csv(\"modified_net_income_data.csv\", index=False, columns=['Ticker'] + list(net_income_df.columns[1:]))\n",
    "\n",
    "print(\"Modified data saved to modified_net_income_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9074beee-bfaa-4245-a997-481e35827239",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Ticker'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply the function to create the new column\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023_PE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_forward_pe)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with only 'Ticker' and '2023_PE' columns\u001b[39;00m\n\u001b[1;32m     23\u001b[0m result_df \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023_PE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Ticker'"
     ]
    }
   ],
   "source": [
    "# Creates a csv file for PE ratio\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get forward PE ratio from Yahoo Finance\n",
    "def get_forward_pe(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get forward PE ratio\n",
    "        forward_pe = ticker_data.info['forwardPE']\n",
    "        return forward_pe\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_PE'] = data['Ticker'].apply(get_forward_pe)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_PE' columns\n",
    "result_df = data[['Ticker', '2023_PE']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_PE.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_PE.csv' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69d97d-6d8f-40e3-ad8b-8ed94d4608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a csv file for GSector \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get GICS sector from Yahoo Finance\n",
    "def get_gsector(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get GICS sector\n",
    "        gsector = ticker_data.info['sector']\n",
    "        return gsector\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['Gsector'] = data['Ticker'].apply(get_gsector)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and 'Gsector' columns\n",
    "result_df = data[['Ticker', 'Gsector']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_Gsector.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_Gsector.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5be66c-fb2b-419e-aa02-ed7df529f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a csv file for Market Cap\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get market capitalization from Yahoo Finance\n",
    "def get_market_cap(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get market cap\n",
    "        market_cap = ticker_data.info['marketCap']\n",
    "        return market_cap\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_MarketCap'] = data['Ticker'].apply(get_market_cap)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_MarketCap' columns\n",
    "result_df = data[['Ticker', '2023_MarketCap']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_MarketCap.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_MarketCap.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab45a8b-4c18-46e4-81e3-976fc1796133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a data frame that calcuates the average PE ratio by Gsector \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with comma as delimiter\n",
    "df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Remove rows with \"inf\" values in 2023_PE column\n",
    "df = df[df['2023_PE'] != float('inf')]\n",
    "\n",
    "# Calculate average PE ratio for each sector\n",
    "avg_pe_by_sector = df.groupby('Gsector')['2023_PE'].mean()\n",
    "\n",
    "# Fill in the Avg_PE_Gsector column based on the calculated averages\n",
    "df['Avg_PE_Gsector'] = df['Gsector'].map(avg_pe_by_sector)\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"cleaned_PE_Gsector_with_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ba829-e6e5-49d6-a2ad-725926416178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame that labels based on market cap size \n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column and create a new column for the category\n",
    "data['Market_Cap_Cat'] = data['2023_MarketCap'].apply(categorize_market_cap)\n",
    "\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "data.to_csv(\"ticker_2023_MarketCap_with_Category.csv\", index=False)\n",
    "\n",
    "print(\"New CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85dc67-b24f-4431-93d7-bae306c69c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines the \"ticker_2023_MarketCap_with_Category.csv\" , \"cleaned_PE_Gsector_with_avg.csv\" and \"\"modified_net_income_data.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "market_cap_data = pd.read_csv(\"ticker_2023_MarketCap_with_Category.csv\")\n",
    "pe_gsector_data = pd.read_csv(\"cleaned_PE_Gsector_with_avg.csv\")\n",
    "net_income_data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Rename the first column in net_income_data to \"Ticker\"\n",
    "net_income_data.rename(columns={net_income_data.columns[0]: 'Ticker'}, inplace=True)\n",
    "\n",
    "# Merge the CSV files based on the 'Ticker' column\n",
    "merged_data = pd.merge(market_cap_data, pe_gsector_data, on='Ticker', how='outer')\n",
    "merged_data = pd.merge(merged_data, net_income_data, on='Ticker', how='outer')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Stalwart_Master.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc1b95-9cb5-4776-866d-a2d783254b1d",
   "metadata": {},
   "source": [
    "# Fast grower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e58640-8fcc-4bf3-b28d-b3984ae0d08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates a csv that combines the Net income growth and PE ratios \n",
    "import pandas as pd\n",
    "\n",
    "# Read the modified_net_income_data.csv\n",
    "net_income_df = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Rename the first column to 'Ticker'\n",
    "net_income_df.rename(columns={net_income_df.columns[0]: 'Ticker'}, inplace=True)\n",
    "\n",
    "# Remove columns 2020_NI, 2021_NI, 2022_NI, 2023_NI\n",
    "net_income_df.drop(columns=['2020_NI', '2021_NI', '2022_NI', '2023_NI'], inplace=True)\n",
    "\n",
    "# Read the ticker_2023_PE.csv\n",
    "pe_df = pd.read_csv(\"ticker_2023_PE.csv\")\n",
    "\n",
    "# Merge the two dataframes on the 'Ticker' column\n",
    "merged_df = pd.merge(net_income_df, pe_df, on='Ticker')\n",
    "\n",
    "# Rename the 2023_PE column\n",
    "merged_df.rename(columns={'PE': '2023_PE'}, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(\"NI_Grow_PE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca16f95-b863-4d45-b05c-5703ef1601af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reades the created file and categoriezes based on ideal growth rate \n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"NI_Grow_PE.csv\")\n",
    "\n",
    "# Calculate average growth rate for each row\n",
    "df['Average_Growth'] = df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Categorize the growth rate\n",
    "def categorize_growth(average_growth):\n",
    "    if 20 <= average_growth <= 25:\n",
    "        return 'Ideal 20% - 25%'\n",
    "    else:\n",
    "        return 'Not Ideal'\n",
    "\n",
    "df['Growth_Rate'] = df['Average_Growth'].apply(categorize_growth)\n",
    "\n",
    "# Save the updated CSV with the name \"Fast_Grower_Master.csv\"\n",
    "df.to_csv(\"Fast_Grower_Master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9856c-c379-4858-b830-6754872addf8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3637ced3-a895-45af-8d0c-3f1c0cf60892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drops all missing values from the master csvs\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Slow_Grow_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Slow_Grow_Master.csv\", index=False)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Fast_Grower_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Fast_Grower_Master.csv\", index=False)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"Stalwart_Master.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"Clean_Stalwart_Master.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8333e-05f4-4ebe-bb28-7c99a6cb1fe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Screen Slow Growers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721a3991-3a43-4b4c-ad35-e049d0c08ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Screens based on dividend growth \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Slow_Grow_Master.csv\")\n",
    "\n",
    "# Filter companies with positive or flat growing dividends for each year\n",
    "filtered_df = df[(df['2020_2021_Div_Grow'] >= 0) & \n",
    "                 (df['2021_2022_Div_Grow'] >= 0) & \n",
    "                 (df['2022_2023_Div_Grow'] >= 0)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Slow_Grow.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b5e99-ba5c-47ae-82ca-6c32f4746e9e",
   "metadata": {},
   "source": [
    "# Screen Stalwarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab947-bfa8-407b-9b54-38ab8bbee515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Stalwart_Master.csv\")\n",
    "\n",
    "# Filter companies with Market_Cap_Cat as 'mega cap' or 'large cap'\n",
    "filtered_df = df[df['Market_Cap_Cat'].isin(['mega-cap', 'large-cap'])]\n",
    "\n",
    "# Calculate average Net Income growth across the three years\n",
    "df['Avg_NI_Growth'] = df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Filter companies where average Net Income growth is between + or - 3% from the industry PE ratio\n",
    "filtered_df = filtered_df[(df['Avg_NI_Growth'] >= 0.97 * df['Avg_PE_Gsector']) & \n",
    "                          (df['Avg_NI_Growth'] <= 1.03 * df['Avg_PE_Gsector'])]\n",
    "\n",
    "# Add a new column for average NI growth\n",
    "filtered_df['Avg_NI_Growth'] = filtered_df[['2020_2021_NI_Grow', '2021_2022_NI_Grow', '2022_2023_NI_Grow']].mean(axis=1)\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Stalwart.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76ea9-f9a8-417a-ac3e-54e10d18f445",
   "metadata": {},
   "source": [
    "# Screen Fast Growers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7eb8a6-bf2e-4e4a-9e35-3cb914b62db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Clean_Fast_Grower_Master.csv\", sep=\"\\t\")\n",
    "\n",
    "# Split the concatenated column names into individual names\n",
    "column_names = df.columns[0].split(',')\n",
    "\n",
    "# Create a new DataFrame with the split values\n",
    "new_df = df.iloc[:, 0].str.split(',', expand=True)\n",
    "\n",
    "# Assign the split column names to the new DataFrame\n",
    "new_df.columns = column_names\n",
    "\n",
    "# Convert 'Average_Growth' and '2023_PE' columns to numeric\n",
    "new_df['Average_Growth'] = pd.to_numeric(new_df['Average_Growth'], errors='coerce')\n",
    "new_df['2023_PE'] = pd.to_numeric(new_df['2023_PE'], errors='coerce')\n",
    "\n",
    "# Filter rows where the average growth rate is between 20% and 25% and within +3 or -3 from the PE ratio\n",
    "filtered_df = new_df[(new_df['Average_Growth'] >= 20) & \n",
    "                     (new_df['Average_Growth'] <= 25) &\n",
    "                     (new_df['Average_Growth'] >= new_df['2023_PE'] - 3) & \n",
    "                     (new_df['Average_Growth'] <= new_df['2023_PE'] + 3)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv(\"Screened_Fast_Grower.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13992658-776a-471c-979a-48c210abdbe5",
   "metadata": {},
   "source": [
    "# Comparative Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1de80d5-d96b-486b-befc-21f9374b9ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2020-12-31    3756.070068\n",
      "2021-12-31    4766.180176\n",
      "2022-12-31    3839.500000\n",
      "2023-12-31    4769.830078\n",
      "Freq: A-DEC, Name: Close, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_sp500_data():\n",
    "    # Define the ticker symbol for S&P 500\n",
    "    ticker_symbol = \"^GSPC\"\n",
    "    \n",
    "    # Define the start and end dates\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    \n",
    "    # Fetch the historical data\n",
    "    sp500_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "    \n",
    "    return sp500_data\n",
    "\n",
    "def get_annual_closing_prices(data):\n",
    "    # Resample the data to annual frequency and extract closing prices\n",
    "    annual_closing_prices = data['Close'].resample('Y').last()\n",
    "    \n",
    "    return annual_closing_prices\n",
    "\n",
    "# Fetch S&P 500 data\n",
    "sp500_data = get_sp500_data()\n",
    "\n",
    "# Get annual closing prices\n",
    "annual_closing_prices = get_annual_closing_prices(sp500_data)\n",
    "\n",
    "# Print the annual closing prices\n",
    "print(annual_closing_prices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b67b1e-90fa-4f3c-beb7-917b17c10c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAGR: 0.0616\n"
     ]
    }
   ],
   "source": [
    "# Calculates the CAGR for the S&P over that time \n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "sp500_data = pd.read_csv(\"S&P_Closing_Price.csv\")\n",
    "\n",
    "# Extract the \"Close\" column\n",
    "closing_prices = sp500_data[\"Close\"]\n",
    "\n",
    "# Calculate CAGR\n",
    "def calculate_cagr(closing_prices):\n",
    "    n = len(closing_prices)\n",
    "    initial_value = closing_prices.iloc[0]\n",
    "    final_value = closing_prices.iloc[-1]\n",
    "    \n",
    "    cagr = ((final_value / initial_value) ** (1/n)) - 1\n",
    "    \n",
    "    return cagr\n",
    "\n",
    "# Calculate CAGR for the provided closing prices\n",
    "cagr = calculate_cagr(closing_prices)\n",
    "\n",
    "# Round the CAGR to 4 decimal places\n",
    "rounded_cagr = round(cagr, 4)\n",
    "\n",
    "print(\"CAGR:\", rounded_cagr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e257bf0-09a2-4b73-9fe6-49c591d84e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2020-12-31    39456.660156\n",
      "2021-12-31    48461.160156\n",
      "2022-12-31    38073.941406\n",
      "2023-12-31    48295.378906\n",
      "Freq: A-DEC, Name: Close, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports the Wilkshire closing values from 2020 - 2023\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_wilshire5000_data():\n",
    "    # Define the ticker symbol for Wilshire 5000\n",
    "    ticker_symbol = \"^W5000\"\n",
    "    \n",
    "    # Define the start and end dates\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    \n",
    "    # Fetch the historical data\n",
    "    wilshire5000_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "    \n",
    "    return wilshire5000_data\n",
    "\n",
    "def get_annual_closing_prices(data):\n",
    "    # Resample the data to annual frequency and extract closing prices\n",
    "    annual_closing_prices = data['Close'].resample('Y').last()\n",
    "    \n",
    "    return annual_closing_prices\n",
    "\n",
    "# Fetch Wilshire 5000 data\n",
    "wilshire5000_data = get_wilshire5000_data()\n",
    "\n",
    "# Get annual closing prices\n",
    "annual_closing_prices_wilshire = get_annual_closing_prices(wilshire5000_data)\n",
    "\n",
    "# Print the annual closing prices\n",
    "print(annual_closing_prices_wilshire)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d260a7e6-2967-445a-805b-725762071079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAGR: 0.0518\n"
     ]
    }
   ],
   "source": [
    "# Calculates the CAGR for the Wilshire 5000 over that time\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "wilshire_data = pd.read_csv(\"Wilshire5000_Closing_Price.csv\")\n",
    "\n",
    "# Extract the \"Close\" column\n",
    "closing_prices = wilshire_data[\"Close\"]\n",
    "\n",
    "# Calculate CAGR\n",
    "def calculate_cagr(closing_prices):\n",
    "    n = len(closing_prices)\n",
    "    initial_value = closing_prices.iloc[0]\n",
    "    final_value = closing_prices.iloc[-1]\n",
    "    \n",
    "    cagr = ((final_value / initial_value) ** (1/n)) - 1\n",
    "    \n",
    "    return cagr\n",
    "\n",
    "# Calculate CAGR for the provided closing prices\n",
    "cagr = calculate_cagr(closing_prices)\n",
    "\n",
    "# Round the CAGR to 4 decimal places\n",
    "rounded_cagr = round(cagr, 4)\n",
    "\n",
    "print(\"CAGR:\", rounded_cagr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15348027-70d2-48d3-ac28-48109488de86",
   "metadata": {},
   "source": [
    "# Get closing prices from screened companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbca8ed-e862-47a9-8fcc-5f909a72a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow Grower Closing Prices and market cap\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Slow_Grow_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Slow_Grow.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8df215-0655-4d0f-b03c-e8d538006737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow Grower market weighted returns \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Slow_Grow_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ef287-7dcb-4c2d-ac84-b298f6d5c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stalwart Closing Prices and Market Cap \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Stalwart_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Stalwart.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6694c-59bf-4251-a2a0-5309af7a0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stalwart Market weighted return \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Stalwart_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17519218-f468-4391-931e-9490e7698862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Grower Closing Prices and Market Cap \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def get_closing_prices_and_market_cap_from_csv(csv_file, years):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract tickers from the \"Ticker\" column\n",
    "    tickers = df['Ticker'].tolist()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for year in years:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                # Fetch stock data using yfinance for the specified year\n",
    "                stock_data = yf.download(ticker, start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "                \n",
    "                # Extract closing price for the last trading day of the year\n",
    "                closing_price = stock_data['Close'].iloc[-1]\n",
    "                \n",
    "                # Fetch market cap\n",
    "                ticker_info = yf.Ticker(ticker)\n",
    "                market_cap = ticker_info.info['marketCap']\n",
    "                \n",
    "                # Prepare data structure\n",
    "                if ticker not in data:\n",
    "                    data[ticker] = {}\n",
    "\n",
    "                data[ticker][f'{year}_Close'] = closing_price\n",
    "                data[ticker][f'{year}_MarketCap'] = market_cap\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker} in {year}: {e}\")\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_df.to_csv('Fast_Grower_Return_MarketCap.csv')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Example usage:\n",
    "csv_file = \"Screened_Fast_Grower.csv\"\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "closing_prices_and_caps = get_closing_prices_and_market_cap_from_csv(csv_file, years)\n",
    "print(closing_prices_and_caps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7213c-aa57-4f27-9210-b6723e69c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Grower Market Weighted Return \n",
    "import pandas as pd\n",
    "\n",
    "def calculate_market_weighted_index(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Initialize a dictionary to store the index prices for each year\n",
    "    index_prices = {}\n",
    "\n",
    "    # For each year, calculate the market-weighted index price\n",
    "    years = ['2020', '2021', '2022', '2023']\n",
    "    for year in years:\n",
    "        # Only consider columns for the specified year\n",
    "        close_column = f'{year}_Close'\n",
    "        cap_column = f'{year}_MarketCap'\n",
    "\n",
    "        # Ensure the data is numeric, handling any potential non-numeric types that could arise from missing data\n",
    "        df[close_column] = pd.to_numeric(df[close_column], errors='coerce')\n",
    "        df[cap_column] = pd.to_numeric(df[cap_column], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of the required data is missing\n",
    "        valid_data = df.dropna(subset=[close_column, cap_column])\n",
    "\n",
    "        # Calculate total market capitalization\n",
    "        total_market_cap = valid_data[cap_column].sum()\n",
    "\n",
    "        # Calculate weighted sum of the closing prices\n",
    "        weighted_sum = (valid_data[close_column] * valid_data[cap_column]).sum()\n",
    "\n",
    "        # Calculate the market-weighted index price\n",
    "        if total_market_cap > 0:  # Avoid division by zero\n",
    "            index_price = weighted_sum / total_market_cap\n",
    "        else:\n",
    "            index_price = None\n",
    "        \n",
    "        # Store the index price in the dictionary\n",
    "        index_prices[year] = index_price\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization and further use\n",
    "    index_prices_df = pd.DataFrame.from_dict(index_prices, orient='index', columns=['Market_Weighted_Index_Price'])\n",
    "    index_prices_df.index.name = 'Year'\n",
    "\n",
    "    return index_prices_df\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'Fast_Grower_Return_MarketCap.csv'\n",
    "index_prices_df = calculate_market_weighted_index(csv_file)\n",
    "print(index_prices_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02558653-8b68-4616-9b56-45162983eb73",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32a0b3-8078-4dd3-8c29-11a95fbc2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data for each index\n",
    "# Data from \n",
    "data = {\n",
    "    'Slow Grower': {2020: 163.438749, 2021: 222.309990, 2022: 185.809803, 2023: 266.864258},\n",
    "    'Stalwart': {2020: 332.570957, 2021: 463.625380, 2022: 390.856517, 2023: 564.568406},\n",
    "    'Fast Grower': {2020: 101.064051, 2021: 143.920488, 2022: 134.847909, 2023: 150.171399},\n",
    "    'S&P 500': {2020: 3756.070068359375, 2021: 4766.180176, 2022: 3839.5, 2023: 4769.830078},\n",
    "    'Wilshire 5000': {2020: 39456.66016, 2021: 48461.16016, 2022: 38073.94141, 2023: 48295.37891}\n",
    "}\n",
    "\n",
    "# Function to calculate CAGR\n",
    "def calculate_cagr(start_value, end_value, periods):\n",
    "    return (end_value / start_value) ** (1 / periods) - 1\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize index values by dividing each value by the starting value (2020 value)\n",
    "normalized_df = df.divide(df.iloc[0])\n",
    "\n",
    "# Calculate CAGR for each series and add to the plot\n",
    "cagr_values = {}\n",
    "for column in normalized_df.columns:\n",
    "    cagr = calculate_cagr(normalized_df[column].iloc[0], normalized_df[column].iloc[-1], len(normalized_df[column]) - 1)\n",
    "    cagr_values[column] = f\"{cagr:.2%}\"\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for column in normalized_df.columns:\n",
    "    ax.plot(normalized_df.index, normalized_df[column], marker='o', label=f\"{column} (CAGR: {cagr_values[column]})\")\n",
    "\n",
    "ax.set_title('Normalized Index Performance Comparison 2020-2023')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Normalized Index Value')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
