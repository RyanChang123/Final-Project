{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7317b2-aad6-4ec1-962f-1b807d6d478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker       2020_NI       2021_NI       2022_NI       2023_NI  \\\n",
      "0       A  7.190000e+08  1.210000e+09  1.254000e+09  1.240000e+09   \n",
      "1      AA -1.700000e+08  4.290000e+08 -1.230000e+08 -6.510000e+08   \n",
      "2     AAL -8.885000e+09 -1.993000e+09  1.270000e+08  8.220000e+08   \n",
      "3    AAME  1.216900e+07  4.281000e+06  1.525000e+06           NaN   \n",
      "4     AAN -2.659120e+08  1.099340e+08 -5.280000e+06  2.823000e+06   \n",
      "5    AAOI -5.845200e+07 -5.416200e+07 -6.639700e+07 -5.604800e+07   \n",
      "6    AAON  7.900900e+07  5.875800e+07  1.003760e+08  1.776230e+08   \n",
      "7     AAP  4.930210e+08  5.966150e+08  4.644020e+08  2.973500e+07   \n",
      "8    AAPL  5.741100e+10  9.468000e+10  9.980300e+10  9.699500e+10   \n",
      "9     AAT  3.558800e+07  3.659300e+07  5.587700e+07  6.469000e+07   \n",
      "10   ABBV  4.616000e+09  1.154200e+10  1.183600e+10  4.863000e+09   \n",
      "11   ABCB  2.619880e+08  3.769130e+08  3.465400e+08  2.691050e+08   \n",
      "12   ABEO -8.423400e+07 -8.493600e+07 -3.969600e+07 -5.418800e+07   \n",
      "13    ABG  2.544000e+08  5.324000e+08  9.973000e+08  6.025000e+08   \n",
      "14   ABIO -9.738000e+06 -1.932200e+07 -9.926000e+06 -5.339000e+06   \n",
      "15    ABM  3.000000e+05  1.263000e+08  2.304000e+08  2.513000e+08   \n",
      "16   ABMD           NaN  2.255250e+08  1.365050e+08           NaN   \n",
      "17    ABR  1.709490e+08  3.393000e+08  3.257830e+08  3.714340e+08   \n",
      "18    ABT  4.495000e+09  7.071000e+09  6.933000e+09  5.723000e+09   \n",
      "19     AC  1.881600e+07  5.920300e+07 -4.890700e+07  3.745100e+07   \n",
      "20    ACA  1.066000e+08  6.960000e+07  2.458000e+08  1.592000e+08   \n",
      "21   ACAD -2.815840e+08 -1.678700e+08 -2.159750e+08 -6.128600e+07   \n",
      "22   ACCO  6.200000e+07  1.019000e+08 -1.320000e+07 -2.180000e+07   \n",
      "23   ACGL  1.405521e+09  2.157000e+09  1.476000e+09  4.443000e+09   \n",
      "24   ACHC -6.721320e+08  1.906350e+08  2.731390e+08 -2.166700e+07   \n",
      "25   ACHV -1.473000e+07 -3.315200e+07 -4.235000e+07 -2.981500e+07   \n",
      "26   ACIW  7.266000e+07  1.277910e+08  1.421770e+08  1.215090e+08   \n",
      "27   ACLS  4.998200e+07  9.865000e+07  1.830790e+08  2.462630e+08   \n",
      "28    ACM -1.863700e+08  1.731850e+08  3.106110e+08  5.533200e+07   \n",
      "29   ACMR  1.878000e+07  3.775700e+07  3.926300e+07  7.734900e+07   \n",
      "30    ACN  5.107839e+09  5.906809e+09  6.877169e+09  6.871557e+09   \n",
      "31   ACNB  1.839400e+07  2.783400e+07  3.575200e+07  3.168800e+07   \n",
      "32   ACOR -9.959400e+07 -1.039540e+08 -6.591600e+07 -2.528540e+08   \n",
      "33   ACRE  2.184000e+07  6.046000e+07  2.978500e+07 -3.886700e+07   \n",
      "34   ACRS -5.101500e+07 -9.086500e+07 -8.690800e+07 -8.848100e+07   \n",
      "35   ACTG  1.092310e+08  1.491970e+08 -1.250650e+08  6.706000e+07   \n",
      "36    ACU  8.098766e+06  1.365568e+07  3.034766e+06  1.779316e+07   \n",
      "37   ADBE  5.260000e+09  4.822000e+09  4.756000e+09  5.428000e+09   \n",
      "38    ADC  9.138100e+07  1.222730e+08  1.524370e+08  1.699590e+08   \n",
      "39    ADI  1.220761e+09  1.390422e+09  2.748561e+09  3.314579e+09   \n",
      "40    ADM  1.772000e+09  2.709000e+09  4.340000e+09  3.483000e+09   \n",
      "41   ADMA -7.574855e+07 -7.164800e+07 -6.590400e+07 -2.823900e+07   \n",
      "42   ADNT -5.470000e+08  1.108000e+09 -1.200000e+08  2.050000e+08   \n",
      "43    ADP  2.466500e+09  2.598500e+09  2.948900e+09  3.412000e+09   \n",
      "44   ADSK           NaN  1.208000e+09  4.970000e+08  8.230000e+08   \n",
      "45    ADT -6.321930e+08 -3.408200e+08  1.326630e+08  4.630090e+08   \n",
      "46   ADTN  2.378000e+06 -8.635000e+06 -2.037000e+06 -2.676880e+08   \n",
      "47   ADUS  3.313300e+07  4.512600e+07  4.602500e+07  6.251600e+07   \n",
      "48   ADVM -1.175070e+08 -1.455400e+08 -1.545360e+08 -1.171650e+08   \n",
      "49   ADXS -3.014600e+07 -4.025400e+07 -3.801300e+07 -4.807200e+07   \n",
      "\n",
      "    2020_2021_NI_Grow  2021_2022_NI_Grow  2022_2023_NI_Grow    2023_PE  \n",
      "0        4.910000e+08       4.400000e+07      -1.400000e+07  22.840984  \n",
      "1        5.990000e+08      -5.520000e+08      -5.280000e+08  15.779736  \n",
      "2        6.892000e+09       2.120000e+09       6.950000e+08   4.311526  \n",
      "3       -7.888000e+06      -2.756000e+06                NaN        NaN  \n",
      "4        3.758460e+08      -1.152140e+08       8.103000e+06  10.367647  \n",
      "5        4.290000e+06      -1.223500e+07       1.034900e+07  11.586206  \n",
      "6       -2.025100e+07       4.161800e+07       7.724700e+07  32.457336  \n",
      "7        1.035940e+08      -1.322130e+08      -4.346670e+08  16.606743  \n",
      "8        3.726900e+10       5.123000e+09      -2.808000e+09  26.229730  \n",
      "9        1.005000e+06       1.928400e+07       8.813000e+06  29.678083  \n",
      "10       6.926000e+09       2.940000e+08      -6.973000e+09  13.418113  \n",
      "11       1.149250e+08      -3.037300e+07      -7.743500e+07   9.824131  \n",
      "12      -7.020000e+05       4.524000e+07      -1.449200e+07  -2.042474  \n",
      "13       2.780000e+08       4.649000e+08      -3.948000e+08   6.843219  \n",
      "14      -9.584000e+06       9.396000e+06       4.587000e+06  -0.402932  \n",
      "15       1.260000e+08       1.041000e+08       2.090000e+07  12.142266  \n",
      "16                NaN      -8.902000e+07                NaN  60.768738  \n",
      "17       1.683510e+08      -1.351700e+07       4.565100e+07   7.581871  \n",
      "18       2.576000e+09      -1.380000e+08      -1.210000e+09  20.742218  \n",
      "19       4.038700e+07      -1.081100e+08       8.635800e+07  12.413129  \n",
      "20      -3.700000e+07       1.762000e+08      -8.660000e+07  19.806122  \n",
      "21       1.137140e+08      -4.810500e+07       1.546890e+08  13.519841  \n",
      "22       3.990000e+07      -1.151000e+08      -8.600000e+06   3.900000  \n",
      "23       7.514790e+08      -6.810000e+08       2.967000e+09  10.694670  \n",
      "24       8.627670e+08       8.250400e+07      -2.948060e+08  18.374690  \n",
      "25      -1.842200e+07      -9.198000e+06       1.253500e+07  -4.977778  \n",
      "26       5.513100e+07       1.438600e+07      -2.066800e+07  20.766080  \n",
      "27       4.866800e+07       8.442900e+07       6.318400e+07  12.484561  \n",
      "28       3.595550e+08       1.374260e+08      -2.552790e+08  18.614645  \n",
      "29       1.897700e+07       1.506000e+06       3.808600e+07  14.302702  \n",
      "30       7.989700e+08       9.703600e+08      -5.612000e+06  23.008537  \n",
      "31       9.440000e+06       7.918000e+06      -4.064000e+06   9.374914  \n",
      "32      -4.360000e+06       3.803800e+07      -1.869380e+08  -0.688542  \n",
      "33       3.862000e+07      -3.067500e+07      -6.865200e+07   8.765822  \n",
      "34      -3.985000e+07       3.957000e+06      -1.573000e+06  -1.481707  \n",
      "35       3.996600e+07      -2.742620e+08       1.921250e+08   8.431034  \n",
      "36       5.556913e+06      -1.062091e+07       1.475839e+07  15.772201  \n",
      "37      -4.380000e+08      -6.600000e+07       6.720000e+08  24.831842  \n",
      "38       3.089200e+07       3.016400e+07       1.752200e+07  30.868280  \n",
      "39       1.696610e+08       1.358139e+09       5.660180e+08  26.884410  \n",
      "40       9.370000e+08       1.631000e+09      -8.570000e+08  10.214036  \n",
      "41       4.100548e+06       5.744000e+06       3.766500e+07  13.250000  \n",
      "42       1.655000e+09      -1.228000e+09       3.250000e+08   7.579747  \n",
      "43       1.320000e+08       3.504000e+08       4.631000e+08  24.383999  \n",
      "44                NaN      -7.110000e+08       3.260000e+08  23.856203  \n",
      "45       2.913730e+08       4.734830e+08       3.303460e+08   7.652941  \n",
      "46      -1.101300e+07       6.598000e+06      -2.656510e+08  11.657894  \n",
      "47       1.199300e+07       8.990000e+05       1.649100e+07  17.815588  \n",
      "48      -2.803300e+07      -8.996000e+06       3.737100e+07  -1.906078  \n",
      "49      -1.010800e+07       2.241000e+06      -1.005900e+07  -0.910959  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Filter the dataset to only include the first 50 tickers\n",
    "data = data.head(50)\n",
    "\n",
    "# Function to get forward PE ratio from Yahoo Finance\n",
    "def get_forward_pe(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get forward PE ratio\n",
    "        forward_pe = ticker_data.info['forwardPE']\n",
    "        return forward_pe\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_PE'] = data['Ticker'].apply(get_forward_pe)\n",
    "\n",
    "# Print the modified dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450df102-33e7-4a1b-b85e-bc251eb15f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker       2020_NI       2021_NI       2022_NI       2023_NI  \\\n",
      "0       A  7.190000e+08  1.210000e+09  1.254000e+09  1.240000e+09   \n",
      "1      AA -1.700000e+08  4.290000e+08 -1.230000e+08 -6.510000e+08   \n",
      "2     AAL -8.885000e+09 -1.993000e+09  1.270000e+08  8.220000e+08   \n",
      "3    AAME  1.216900e+07  4.281000e+06  1.525000e+06           NaN   \n",
      "4     AAN -2.659120e+08  1.099340e+08 -5.280000e+06  2.823000e+06   \n",
      "5    AAOI -5.845200e+07 -5.416200e+07 -6.639700e+07 -5.604800e+07   \n",
      "6    AAON  7.900900e+07  5.875800e+07  1.003760e+08  1.776230e+08   \n",
      "7     AAP  4.930210e+08  5.966150e+08  4.644020e+08  2.973500e+07   \n",
      "8    AAPL  5.741100e+10  9.468000e+10  9.980300e+10  9.699500e+10   \n",
      "9     AAT  3.558800e+07  3.659300e+07  5.587700e+07  6.469000e+07   \n",
      "10   ABBV  4.616000e+09  1.154200e+10  1.183600e+10  4.863000e+09   \n",
      "11   ABCB  2.619880e+08  3.769130e+08  3.465400e+08  2.691050e+08   \n",
      "12   ABEO -8.423400e+07 -8.493600e+07 -3.969600e+07 -5.418800e+07   \n",
      "13    ABG  2.544000e+08  5.324000e+08  9.973000e+08  6.025000e+08   \n",
      "14   ABIO -9.738000e+06 -1.932200e+07 -9.926000e+06 -5.339000e+06   \n",
      "15    ABM  3.000000e+05  1.263000e+08  2.304000e+08  2.513000e+08   \n",
      "16   ABMD           NaN  2.255250e+08  1.365050e+08           NaN   \n",
      "17    ABR  1.709490e+08  3.393000e+08  3.257830e+08  3.714340e+08   \n",
      "18    ABT  4.495000e+09  7.071000e+09  6.933000e+09  5.723000e+09   \n",
      "19     AC  1.881600e+07  5.920300e+07 -4.890700e+07  3.745100e+07   \n",
      "20    ACA  1.066000e+08  6.960000e+07  2.458000e+08  1.592000e+08   \n",
      "21   ACAD -2.815840e+08 -1.678700e+08 -2.159750e+08 -6.128600e+07   \n",
      "22   ACCO  6.200000e+07  1.019000e+08 -1.320000e+07 -2.180000e+07   \n",
      "23   ACGL  1.405521e+09  2.157000e+09  1.476000e+09  4.443000e+09   \n",
      "24   ACHC -6.721320e+08  1.906350e+08  2.731390e+08 -2.166700e+07   \n",
      "25   ACHV -1.473000e+07 -3.315200e+07 -4.235000e+07 -2.981500e+07   \n",
      "26   ACIW  7.266000e+07  1.277910e+08  1.421770e+08  1.215090e+08   \n",
      "27   ACLS  4.998200e+07  9.865000e+07  1.830790e+08  2.462630e+08   \n",
      "28    ACM -1.863700e+08  1.731850e+08  3.106110e+08  5.533200e+07   \n",
      "29   ACMR  1.878000e+07  3.775700e+07  3.926300e+07  7.734900e+07   \n",
      "30    ACN  5.107839e+09  5.906809e+09  6.877169e+09  6.871557e+09   \n",
      "31   ACNB  1.839400e+07  2.783400e+07  3.575200e+07  3.168800e+07   \n",
      "32   ACOR -9.959400e+07 -1.039540e+08 -6.591600e+07 -2.528540e+08   \n",
      "33   ACRE  2.184000e+07  6.046000e+07  2.978500e+07 -3.886700e+07   \n",
      "34   ACRS -5.101500e+07 -9.086500e+07 -8.690800e+07 -8.848100e+07   \n",
      "35   ACTG  1.092310e+08  1.491970e+08 -1.250650e+08  6.706000e+07   \n",
      "36    ACU  8.098766e+06  1.365568e+07  3.034766e+06  1.779316e+07   \n",
      "37   ADBE  5.260000e+09  4.822000e+09  4.756000e+09  5.428000e+09   \n",
      "38    ADC  9.138100e+07  1.222730e+08  1.524370e+08  1.699590e+08   \n",
      "39    ADI  1.220761e+09  1.390422e+09  2.748561e+09  3.314579e+09   \n",
      "40    ADM  1.772000e+09  2.709000e+09  4.340000e+09  3.483000e+09   \n",
      "41   ADMA -7.574855e+07 -7.164800e+07 -6.590400e+07 -2.823900e+07   \n",
      "42   ADNT -5.470000e+08  1.108000e+09 -1.200000e+08  2.050000e+08   \n",
      "43    ADP  2.466500e+09  2.598500e+09  2.948900e+09  3.412000e+09   \n",
      "44   ADSK           NaN  1.208000e+09  4.970000e+08  8.230000e+08   \n",
      "45    ADT -6.321930e+08 -3.408200e+08  1.326630e+08  4.630090e+08   \n",
      "46   ADTN  2.378000e+06 -8.635000e+06 -2.037000e+06 -2.676880e+08   \n",
      "47   ADUS  3.313300e+07  4.512600e+07  4.602500e+07  6.251600e+07   \n",
      "48   ADVM -1.175070e+08 -1.455400e+08 -1.545360e+08 -1.171650e+08   \n",
      "49   ADXS -3.014600e+07 -4.025400e+07 -3.801300e+07 -4.807200e+07   \n",
      "\n",
      "    2020_2021_NI_Grow  2021_2022_NI_Grow  2022_2023_NI_Grow  \\\n",
      "0        4.910000e+08       4.400000e+07      -1.400000e+07   \n",
      "1        5.990000e+08      -5.520000e+08      -5.280000e+08   \n",
      "2        6.892000e+09       2.120000e+09       6.950000e+08   \n",
      "3       -7.888000e+06      -2.756000e+06                NaN   \n",
      "4        3.758460e+08      -1.152140e+08       8.103000e+06   \n",
      "5        4.290000e+06      -1.223500e+07       1.034900e+07   \n",
      "6       -2.025100e+07       4.161800e+07       7.724700e+07   \n",
      "7        1.035940e+08      -1.322130e+08      -4.346670e+08   \n",
      "8        3.726900e+10       5.123000e+09      -2.808000e+09   \n",
      "9        1.005000e+06       1.928400e+07       8.813000e+06   \n",
      "10       6.926000e+09       2.940000e+08      -6.973000e+09   \n",
      "11       1.149250e+08      -3.037300e+07      -7.743500e+07   \n",
      "12      -7.020000e+05       4.524000e+07      -1.449200e+07   \n",
      "13       2.780000e+08       4.649000e+08      -3.948000e+08   \n",
      "14      -9.584000e+06       9.396000e+06       4.587000e+06   \n",
      "15       1.260000e+08       1.041000e+08       2.090000e+07   \n",
      "16                NaN      -8.902000e+07                NaN   \n",
      "17       1.683510e+08      -1.351700e+07       4.565100e+07   \n",
      "18       2.576000e+09      -1.380000e+08      -1.210000e+09   \n",
      "19       4.038700e+07      -1.081100e+08       8.635800e+07   \n",
      "20      -3.700000e+07       1.762000e+08      -8.660000e+07   \n",
      "21       1.137140e+08      -4.810500e+07       1.546890e+08   \n",
      "22       3.990000e+07      -1.151000e+08      -8.600000e+06   \n",
      "23       7.514790e+08      -6.810000e+08       2.967000e+09   \n",
      "24       8.627670e+08       8.250400e+07      -2.948060e+08   \n",
      "25      -1.842200e+07      -9.198000e+06       1.253500e+07   \n",
      "26       5.513100e+07       1.438600e+07      -2.066800e+07   \n",
      "27       4.866800e+07       8.442900e+07       6.318400e+07   \n",
      "28       3.595550e+08       1.374260e+08      -2.552790e+08   \n",
      "29       1.897700e+07       1.506000e+06       3.808600e+07   \n",
      "30       7.989700e+08       9.703600e+08      -5.612000e+06   \n",
      "31       9.440000e+06       7.918000e+06      -4.064000e+06   \n",
      "32      -4.360000e+06       3.803800e+07      -1.869380e+08   \n",
      "33       3.862000e+07      -3.067500e+07      -6.865200e+07   \n",
      "34      -3.985000e+07       3.957000e+06      -1.573000e+06   \n",
      "35       3.996600e+07      -2.742620e+08       1.921250e+08   \n",
      "36       5.556913e+06      -1.062091e+07       1.475839e+07   \n",
      "37      -4.380000e+08      -6.600000e+07       6.720000e+08   \n",
      "38       3.089200e+07       3.016400e+07       1.752200e+07   \n",
      "39       1.696610e+08       1.358139e+09       5.660180e+08   \n",
      "40       9.370000e+08       1.631000e+09      -8.570000e+08   \n",
      "41       4.100548e+06       5.744000e+06       3.766500e+07   \n",
      "42       1.655000e+09      -1.228000e+09       3.250000e+08   \n",
      "43       1.320000e+08       3.504000e+08       4.631000e+08   \n",
      "44                NaN      -7.110000e+08       3.260000e+08   \n",
      "45       2.913730e+08       4.734830e+08       3.303460e+08   \n",
      "46      -1.101300e+07       6.598000e+06      -2.656510e+08   \n",
      "47       1.199300e+07       8.990000e+05       1.649100e+07   \n",
      "48      -2.803300e+07      -8.996000e+06       3.737100e+07   \n",
      "49      -1.010800e+07       2.241000e+06      -1.005900e+07   \n",
      "\n",
      "               Gsector  \n",
      "0           Healthcare  \n",
      "1      Basic Materials  \n",
      "2          Industrials  \n",
      "3   Financial Services  \n",
      "4          Industrials  \n",
      "5           Technology  \n",
      "6          Industrials  \n",
      "7    Consumer Cyclical  \n",
      "8           Technology  \n",
      "9          Real Estate  \n",
      "10          Healthcare  \n",
      "11  Financial Services  \n",
      "12          Healthcare  \n",
      "13   Consumer Cyclical  \n",
      "14          Healthcare  \n",
      "15         Industrials  \n",
      "16                None  \n",
      "17         Real Estate  \n",
      "18          Healthcare  \n",
      "19  Financial Services  \n",
      "20         Industrials  \n",
      "21          Healthcare  \n",
      "22         Industrials  \n",
      "23  Financial Services  \n",
      "24          Healthcare  \n",
      "25          Healthcare  \n",
      "26          Technology  \n",
      "27          Technology  \n",
      "28         Industrials  \n",
      "29          Technology  \n",
      "30          Technology  \n",
      "31  Financial Services  \n",
      "32          Healthcare  \n",
      "33         Real Estate  \n",
      "34          Healthcare  \n",
      "35         Industrials  \n",
      "36  Consumer Defensive  \n",
      "37          Technology  \n",
      "38         Real Estate  \n",
      "39          Technology  \n",
      "40  Consumer Defensive  \n",
      "41          Healthcare  \n",
      "42   Consumer Cyclical  \n",
      "43         Industrials  \n",
      "44          Technology  \n",
      "45         Industrials  \n",
      "46          Technology  \n",
      "47          Healthcare  \n",
      "48          Healthcare  \n",
      "49          Healthcare  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Filter the dataset to only include the first 50 tickers\n",
    "data = data.head(50)\n",
    "\n",
    "# Function to get GICS sector from Yahoo Finance\n",
    "def get_gsector(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get GICS sector\n",
    "        gsector = ticker_data.info['sector']\n",
    "        return gsector\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['Gsector'] = data['Ticker'].apply(get_gsector)\n",
    "\n",
    "# Print the modified dataset\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6f7b5c-4813-4f99-8f9a-e209cccaff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker       2020_NI       2021_NI       2022_NI       2023_NI  \\\n",
      "0       A  7.190000e+08  1.210000e+09  1.254000e+09  1.240000e+09   \n",
      "1      AA -1.700000e+08  4.290000e+08 -1.230000e+08 -6.510000e+08   \n",
      "2     AAL -8.885000e+09 -1.993000e+09  1.270000e+08  8.220000e+08   \n",
      "3    AAME  1.216900e+07  4.281000e+06  1.525000e+06           NaN   \n",
      "4     AAN -2.659120e+08  1.099340e+08 -5.280000e+06  2.823000e+06   \n",
      "5    AAOI -5.845200e+07 -5.416200e+07 -6.639700e+07 -5.604800e+07   \n",
      "6    AAON  7.900900e+07  5.875800e+07  1.003760e+08  1.776230e+08   \n",
      "7     AAP  4.930210e+08  5.966150e+08  4.644020e+08  2.973500e+07   \n",
      "8    AAPL  5.741100e+10  9.468000e+10  9.980300e+10  9.699500e+10   \n",
      "9     AAT  3.558800e+07  3.659300e+07  5.587700e+07  6.469000e+07   \n",
      "10   ABBV  4.616000e+09  1.154200e+10  1.183600e+10  4.863000e+09   \n",
      "11   ABCB  2.619880e+08  3.769130e+08  3.465400e+08  2.691050e+08   \n",
      "12   ABEO -8.423400e+07 -8.493600e+07 -3.969600e+07 -5.418800e+07   \n",
      "13    ABG  2.544000e+08  5.324000e+08  9.973000e+08  6.025000e+08   \n",
      "14   ABIO -9.738000e+06 -1.932200e+07 -9.926000e+06 -5.339000e+06   \n",
      "15    ABM  3.000000e+05  1.263000e+08  2.304000e+08  2.513000e+08   \n",
      "16   ABMD           NaN  2.255250e+08  1.365050e+08           NaN   \n",
      "17    ABR  1.709490e+08  3.393000e+08  3.257830e+08  3.714340e+08   \n",
      "18    ABT  4.495000e+09  7.071000e+09  6.933000e+09  5.723000e+09   \n",
      "19     AC  1.881600e+07  5.920300e+07 -4.890700e+07  3.745100e+07   \n",
      "20    ACA  1.066000e+08  6.960000e+07  2.458000e+08  1.592000e+08   \n",
      "21   ACAD -2.815840e+08 -1.678700e+08 -2.159750e+08 -6.128600e+07   \n",
      "22   ACCO  6.200000e+07  1.019000e+08 -1.320000e+07 -2.180000e+07   \n",
      "23   ACGL  1.405521e+09  2.157000e+09  1.476000e+09  4.443000e+09   \n",
      "24   ACHC -6.721320e+08  1.906350e+08  2.731390e+08 -2.166700e+07   \n",
      "25   ACHV -1.473000e+07 -3.315200e+07 -4.235000e+07 -2.981500e+07   \n",
      "26   ACIW  7.266000e+07  1.277910e+08  1.421770e+08  1.215090e+08   \n",
      "27   ACLS  4.998200e+07  9.865000e+07  1.830790e+08  2.462630e+08   \n",
      "28    ACM -1.863700e+08  1.731850e+08  3.106110e+08  5.533200e+07   \n",
      "29   ACMR  1.878000e+07  3.775700e+07  3.926300e+07  7.734900e+07   \n",
      "30    ACN  5.107839e+09  5.906809e+09  6.877169e+09  6.871557e+09   \n",
      "31   ACNB  1.839400e+07  2.783400e+07  3.575200e+07  3.168800e+07   \n",
      "32   ACOR -9.959400e+07 -1.039540e+08 -6.591600e+07 -2.528540e+08   \n",
      "33   ACRE  2.184000e+07  6.046000e+07  2.978500e+07 -3.886700e+07   \n",
      "34   ACRS -5.101500e+07 -9.086500e+07 -8.690800e+07 -8.848100e+07   \n",
      "35   ACTG  1.092310e+08  1.491970e+08 -1.250650e+08  6.706000e+07   \n",
      "36    ACU  8.098766e+06  1.365568e+07  3.034766e+06  1.779316e+07   \n",
      "37   ADBE  5.260000e+09  4.822000e+09  4.756000e+09  5.428000e+09   \n",
      "38    ADC  9.138100e+07  1.222730e+08  1.524370e+08  1.699590e+08   \n",
      "39    ADI  1.220761e+09  1.390422e+09  2.748561e+09  3.314579e+09   \n",
      "40    ADM  1.772000e+09  2.709000e+09  4.340000e+09  3.483000e+09   \n",
      "41   ADMA -7.574855e+07 -7.164800e+07 -6.590400e+07 -2.823900e+07   \n",
      "42   ADNT -5.470000e+08  1.108000e+09 -1.200000e+08  2.050000e+08   \n",
      "43    ADP  2.466500e+09  2.598500e+09  2.948900e+09  3.412000e+09   \n",
      "44   ADSK           NaN  1.208000e+09  4.970000e+08  8.230000e+08   \n",
      "45    ADT -6.321930e+08 -3.408200e+08  1.326630e+08  4.630090e+08   \n",
      "46   ADTN  2.378000e+06 -8.635000e+06 -2.037000e+06 -2.676880e+08   \n",
      "47   ADUS  3.313300e+07  4.512600e+07  4.602500e+07  6.251600e+07   \n",
      "48   ADVM -1.175070e+08 -1.455400e+08 -1.545360e+08 -1.171650e+08   \n",
      "49   ADXS -3.014600e+07 -4.025400e+07 -3.801300e+07 -4.807200e+07   \n",
      "\n",
      "    2020_2021_NI_Grow  2021_2022_NI_Grow  2022_2023_NI_Grow  2023_MarketCap  \n",
      "0        4.910000e+08       4.400000e+07      -1.400000e+07     40831352832  \n",
      "1        5.990000e+08      -5.520000e+08      -5.280000e+08      6440781312  \n",
      "2        6.892000e+09       2.120000e+09       6.950000e+08      9086846976  \n",
      "3       -7.888000e+06      -2.756000e+06                NaN        38368564  \n",
      "4        3.758460e+08      -1.152140e+08       8.103000e+06       215909776  \n",
      "5        4.290000e+06      -1.223500e+07       1.034900e+07       387436832  \n",
      "6       -2.025100e+07       4.161800e+07       7.724700e+07      7801141760  \n",
      "7        1.035940e+08      -1.322130e+08      -4.346670e+08      4411132928  \n",
      "8        3.726900e+10       5.123000e+09      -2.808000e+09   2695306412032  \n",
      "9        1.005000e+06       1.928400e+07       8.813000e+06      1669862400  \n",
      "10       6.926000e+09       2.940000e+08      -6.973000e+09    287164006400  \n",
      "11       1.149250e+08      -3.037300e+07      -7.743500e+07      3320299008  \n",
      "12      -7.020000e+05       4.524000e+07      -1.449200e+07       106684504  \n",
      "13       2.780000e+08       4.649000e+08      -3.948000e+08      4305554432  \n",
      "14      -9.584000e+06       9.396000e+06       4.587000e+06        51418000  \n",
      "15       1.260000e+08       1.041000e+08       2.090000e+07      2783560192  \n",
      "16                NaN      -8.902000e+07                NaN     17180649472  \n",
      "17       1.683510e+08      -1.351700e+07       4.565100e+07      2457211392  \n",
      "18       2.576000e+09      -1.380000e+08      -1.210000e+09    184961515520  \n",
      "19       4.038700e+07      -1.081100e+08       8.635800e+07       688386176  \n",
      "20      -3.700000e+07       1.762000e+08      -8.660000e+07      3773737472  \n",
      "21       1.137140e+08      -4.810500e+07       1.546890e+08      2815059456  \n",
      "22       3.990000e+07      -1.151000e+08      -8.600000e+06       462775488  \n",
      "23       7.514790e+08      -6.810000e+08       2.967000e+09     34627993600  \n",
      "24       8.627670e+08       8.250400e+07      -2.948060e+08      6844286464  \n",
      "25      -1.842200e+07      -9.198000e+06       1.253500e+07       153448512  \n",
      "26       5.513100e+07       1.438600e+07      -2.066800e+07      3764650240  \n",
      "27       4.866800e+07       8.442900e+07       6.318400e+07      3427195904  \n",
      "28       3.595550e+08       1.374260e+08      -2.552790e+08     12828423168  \n",
      "29       1.897700e+07       1.506000e+06       3.808600e+07      1640747520  \n",
      "30       7.989700e+08       9.703600e+08      -5.612000e+06    189857300480  \n",
      "31       9.440000e+06       7.918000e+06      -4.064000e+06       281803552  \n",
      "32      -4.360000e+06       3.803800e+07      -1.869380e+08          821028  \n",
      "33       3.862000e+07      -3.067500e+07      -6.865200e+07       375548576  \n",
      "34      -3.985000e+07       3.957000e+06      -1.573000e+06        86528504  \n",
      "35       3.996600e+07      -2.742620e+08       1.921250e+08       489107552  \n",
      "36       5.556913e+06      -1.062091e+07       1.475839e+07       148906416  \n",
      "37      -4.380000e+08      -6.600000e+07       6.720000e+08    211664322560  \n",
      "38       3.089200e+07       3.016400e+07       1.752200e+07      5804995584  \n",
      "39       1.696610e+08       1.358139e+09       5.660180e+08    100907360256  \n",
      "40       9.370000e+08       1.631000e+09      -8.570000e+08     29222735872  \n",
      "41       4.100548e+06       5.744000e+06       3.766500e+07      1533171712  \n",
      "42       1.655000e+09      -1.228000e+09       3.250000e+08      2733727232  \n",
      "43       1.320000e+08       3.504000e+08       4.631000e+08    100200144896  \n",
      "44                NaN      -7.110000e+08       3.260000e+08     46498705408  \n",
      "45       2.913730e+08       4.734830e+08       3.303460e+08      5921982976  \n",
      "46      -1.101300e+07       6.598000e+06      -2.656510e+08       350482976  \n",
      "47       1.199300e+07       8.990000e+05       1.649100e+07      1534060800  \n",
      "48      -2.803300e+07      -8.996000e+06       3.737100e+07       214813216  \n",
      "49      -1.010800e+07       2.241000e+06      -1.005900e+07        28324944  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Filter the dataset to only include the first 50 tickers\n",
    "data = data.head(50)\n",
    "\n",
    "# Function to get market capitalization from Yahoo Finance\n",
    "def get_market_cap(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get market cap\n",
    "        market_cap = ticker_data.info['marketCap']\n",
    "        return market_cap\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_MarketCap'] = data['Ticker'].apply(get_market_cap)\n",
    "\n",
    "# Print the modified dataset\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8463d324-2e9f-4e83-9558-47b76b6444ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ticker_2023_PE.csv' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get forward PE ratio from Yahoo Finance\n",
    "def get_forward_pe(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get forward PE ratio\n",
    "        forward_pe = ticker_data.info['forwardPE']\n",
    "        return forward_pe\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_PE'] = data['Ticker'].apply(get_forward_pe)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_PE' columns\n",
    "result_df = data[['Ticker', '2023_PE']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_PE.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_PE.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb03da5-9efd-459b-a503-6b63697545e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ticker_Gsector.csv' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get GICS sector from Yahoo Finance\n",
    "def get_gsector(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get GICS sector\n",
    "        gsector = ticker_data.info['sector']\n",
    "        return gsector\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['Gsector'] = data['Ticker'].apply(get_gsector)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and 'Gsector' columns\n",
    "result_df = data[['Ticker', 'Gsector']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_Gsector.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_Gsector.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a919b0-dcb5-491b-8ee4-0cbbfb05004b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ticker_2023_MarketCap.csv' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Function to get market capitalization from Yahoo Finance\n",
    "def get_market_cap(ticker):\n",
    "    try:\n",
    "        # Fetch ticker data from Yahoo Finance\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        # Get market cap\n",
    "        market_cap = ticker_data.info['marketCap']\n",
    "        return market_cap\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['2023_MarketCap'] = data['Ticker'].apply(get_market_cap)\n",
    "\n",
    "# Create a new DataFrame with only 'Ticker' and '2023_MarketCap' columns\n",
    "result_df = data[['Ticker', '2023_MarketCap']]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "result_df.to_csv(\"ticker_2023_MarketCap.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the save\n",
    "print(\"CSV file 'ticker_2023_MarketCap.csv' has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e167b31e-9518-4b35-8657-174c2085d018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to PE_Gsector.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from CSV files\n",
    "ticker_2023_PE_df = pd.read_csv(\"ticker_2023_PE.csv\")\n",
    "ticker_Gsector_df = pd.read_csv(\"ticker_Gsector.csv\")\n",
    "\n",
    "# Merge the two DataFrames based on the 'Ticker' column\n",
    "merged_df = pd.merge(ticker_2023_PE_df, ticker_Gsector_df, on=\"Ticker\")\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file labeled as \"PE_Gsector.csv\"\n",
    "merged_df.to_csv(\"PE_Gsector.csv\", index=False)\n",
    "\n",
    "print(\"Merged data saved to PE_Gsector.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be039db2-afef-40cf-b6b7-774049b732dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to PE_Gsector_with_avg.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector.csv\")\n",
    "\n",
    "# Calculate average PE ratio by Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "avg_pe_by_sector.rename(columns={'2023_PE': 'Avg_PE_Gsector'}, inplace=True)\n",
    "\n",
    "# Merge the average PE ratios back into the original DataFrame based on Gsector\n",
    "merged_df = pd.merge(merged_df, avg_pe_by_sector, on=\"Gsector\")\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"PE_Gsector_with_avg.csv\", index=False)\n",
    "\n",
    "print(\"Modified data saved to PE_Gsector_with_avg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ed779b-4dc2-421f-b763-d6b0b721fffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to PE_Gsector_with_avg.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file and drop rows with missing values\n",
    "merged_df = pd.read_csv(\"PE_Gsector.csv\").dropna()\n",
    "\n",
    "# Calculate average PE ratio by Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "avg_pe_by_sector.rename(columns={'2023_PE': 'Avg_PE_Gsector'}, inplace=True)\n",
    "\n",
    "# Merge the average PE ratios back into the original DataFrame based on Gsector\n",
    "merged_df = pd.merge(merged_df, avg_pe_by_sector, on=\"Gsector\")\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"PE_Gsector_with_avg.csv\", index=False)\n",
    "\n",
    "print(\"Modified data saved to PE_Gsector_with_avg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81ddfff-1e99-4d2f-95f3-3474ea9354ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector.csv\")\n",
    "\n",
    "# Define the list of Gsectors\n",
    "gsectors = [\"Healthcare\", \"Basic Materials\", \"Industrials\", \"Financial Services\", \n",
    "            \"Technology\", \"Consumer Cyclical\", \"Real Estate\", \"Consumer Defensive\", \n",
    "            \"Energy\", \"Utilities\", \"Communication Services\"]\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "# Filter only the desired Gsectors\n",
    "avg_pe_by_sector = avg_pe_by_sector[avg_pe_by_sector['Gsector'].isin(gsectors)]\n",
    "\n",
    "# Print the average PE ratio for each Gsector\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a91766-18e9-423e-a5ef-6bc51213496e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector.csv\")\n",
    "\n",
    "# Define the list of Gsectors\n",
    "gsectors = [\"Healthcare\", \"Basic Materials\", \"Industrials\", \"Financial Services\", \n",
    "            \"Technology\", \"Consumer Cyclical\", \"Real Estate\", \"Consumer Defensive\", \n",
    "            \"Energy\", \"Utilities\", \"Communication Services\"]\n",
    "\n",
    "# Remove rows with missing values in the '2023_PE' column\n",
    "merged_df.dropna(subset=['2023_PE'], inplace=True)\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "# Filter only the desired Gsectors\n",
    "avg_pe_by_sector = avg_pe_by_sector[avg_pe_by_sector['Gsector'].isin(gsectors)]\n",
    "\n",
    "# Print the average PE ratio for each Gsector\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51201211-cb2e-41f0-8627-78e062563233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sectors: set()\n"
     ]
    }
   ],
   "source": [
    "# Check which sectors are missing\n",
    "missing_sectors = set(gsectors) - set(avg_pe_by_sector['Gsector'])\n",
    "print(\"Missing sectors:\", missing_sectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399019a9-45e9-4fe0-9dd8-961176b84a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Drop rows with missing values in the '2023_PE' column\n",
    "merged_df.dropna(subset=['2023_PE'], inplace=True)\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3706abc5-7534-4ce9-8430-1f1df67083cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Convert '2023_PE' column to numeric, coerce errors to NaN\n",
    "merged_df['2023_PE'] = pd.to_numeric(merged_df['2023_PE'], errors='coerce')\n",
    "\n",
    "# Drop rows with non-float values in the '2023_PE' column\n",
    "merged_df.dropna(subset=['2023_PE'], inplace=True)\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aabad998-551f-49c2-a70a-4bc366755f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Convert '2023_PE' column to numeric, coerce errors to NaN\n",
    "merged_df['2023_PE'] = pd.to_numeric(merged_df['2023_PE'], errors='coerce')\n",
    "\n",
    "# Drop rows with non-float values in the '2023_PE' column\n",
    "merged_df.dropna(subset=['2023_PE'], inplace=True)\n",
    "\n",
    "# Drop rows with no PE value\n",
    "merged_df = merged_df.dropna(subset=['2023_PE'])\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9379fba-7a42-4825-8c88-69cfe5062e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Gsector    2023_PE\n",
      "0          Basic Materials  12.553481\n",
      "1   Communication Services        NaN\n",
      "2        Consumer Cyclical  14.477286\n",
      "3       Consumer Defensive  16.978318\n",
      "4                   Energy  12.364548\n",
      "5       Financial Services  12.598470\n",
      "6               Healthcare        NaN\n",
      "7              Industrials        NaN\n",
      "8              Real Estate        NaN\n",
      "9               Technology        NaN\n",
      "10               Utilities  17.248462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from the merged CSV file\n",
    "merged_df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Replace non-numeric values such as 'inf' and empty strings with NaN\n",
    "merged_df['2023_PE'] = pd.to_numeric(merged_df['2023_PE'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the '2023_PE' column\n",
    "merged_df.dropna(subset=['2023_PE'], inplace=True)\n",
    "\n",
    "# Calculate average PE ratio for each Gsector\n",
    "avg_pe_by_sector = merged_df.groupby('Gsector')['2023_PE'].mean().reset_index()\n",
    "\n",
    "print(avg_pe_by_sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46f5b65c-1d4a-4686-a681-0ca984d05c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2023_PE_Gsector_with_avg.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate average PE ratio\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m average_pe \u001b[38;5;241m=\u001b[39m calculate_average_pe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023_PE_Gsector_with_avg.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Print result\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_pe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mcalculate_average_pe\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_average_pe\u001b[39m(filename):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Initialize variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2023_PE_Gsector_with_avg.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Function to calculate average PE ratio\n",
    "def calculate_average_pe(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        \n",
    "        # Initialize variables\n",
    "        total_pe = 0\n",
    "        valid_count = 0\n",
    "\n",
    "        # Iterate through rows\n",
    "        for row in reader:\n",
    "            pe_str = row['2023_PE']\n",
    "            # Check if PE ratio is valid (not 'inf' or '-')\n",
    "            if pe_str != 'inf' and pe_str != '-':\n",
    "                total_pe += float(pe_str)\n",
    "                valid_count += 1\n",
    "\n",
    "        # Calculate average PE ratio\n",
    "        if valid_count > 0:\n",
    "            average_pe = total_pe / valid_count\n",
    "            return average_pe\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Calculate average PE ratio\n",
    "average_pe = calculate_average_pe('2023_PE_Gsector_with_avg.csv')\n",
    "\n",
    "# Print result\n",
    "if average_pe is not None:\n",
    "    print(\"Average PE ratio for 2023:\", average_pe)\n",
    "else:\n",
    "    print(\"No valid PE ratios found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9888fe13-f379-4090-b263-22d10b962cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Gsector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPE_Gsector_with_avg.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate average PE ratio for each sector\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m avg_pe_by_sector \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGsector\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023_PE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fill in the Avg_PE_Gsector column based on the calculated averages\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_PE_Gsector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGsector\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(avg_pe_by_sector)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8253\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8254\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[1;32m   8255\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   8256\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   8257\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[1;32m   8258\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   8259\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   8260\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[1;32m   8261\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[1;32m   8262\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[1;32m    932\u001b[0m         obj,\n\u001b[1;32m    933\u001b[0m         keys,\n\u001b[1;32m    934\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    935\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m    936\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    937\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[1;32m    938\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[1;32m    939\u001b[0m     )\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Gsector'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"PE_Gsector_with_avg.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Calculate average PE ratio for each sector\n",
    "avg_pe_by_sector = df.groupby('Gsector')['2023_PE'].mean()\n",
    "\n",
    "# Fill in the Avg_PE_Gsector column based on the calculated averages\n",
    "df['Avg_PE_Gsector'] = df['Gsector'].map(avg_pe_by_sector)\n",
    "\n",
    "# Drop rows with improper values (e.g., negative PE ratios)\n",
    "df = df[df['2023_PE'] >= 0]\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"cleaned_PE_Gsector_with_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bc45e1f-bff2-45f9-86d0-ab7f18f6fd6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ticker,2023_PE,Gsector,Avg_PE_Gsector'], dtype='object')\n",
      "  Ticker,2023_PE,Gsector,Avg_PE_Gsector\n",
      "0               A,22.831148,Healthcare,\n",
      "1            ABBV,13.412252,Healthcare,\n",
      "2           ABEO,-2.0078948,Healthcare,\n",
      "3          ABIO,-0.40292045,Healthcare,\n",
      "4             ABT,20.723736,Healthcare,\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e47edc22-459a-4007-9d32-792909a309f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file with comma as delimiter\n",
    "df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Calculate average PE ratio for each sector\n",
    "avg_pe_by_sector = df.groupby('Gsector')['2023_PE'].mean()\n",
    "\n",
    "# Fill in the Avg_PE_Gsector column based on the calculated averages\n",
    "df['Avg_PE_Gsector'] = df['Gsector'].map(avg_pe_by_sector)\n",
    "\n",
    "# Drop rows with improper values (e.g., negative PE ratios)\n",
    "df = df[df['2023_PE'] >= 0]\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"cleaned_PE_Gsector_with_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2bd14b0-45aa-4ab2-a459-4b23e0bef21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with comma as delimiter\n",
    "df = pd.read_csv(\"PE_Gsector_with_avg.csv\")\n",
    "\n",
    "# Remove rows with \"inf\" values in 2023_PE column\n",
    "df = df[df['2023_PE'] != float('inf')]\n",
    "\n",
    "# Calculate average PE ratio for each sector\n",
    "avg_pe_by_sector = df.groupby('Gsector')['2023_PE'].mean()\n",
    "\n",
    "# Fill in the Avg_PE_Gsector column based on the calculated averages\n",
    "df['Avg_PE_Gsector'] = df['Gsector'].map(avg_pe_by_sector)\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"cleaned_PE_Gsector_with_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63cb655a-4c36-44a3-85af-6592c4efcfec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Market Cap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Market Cap'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Apply categorization to the market cap column\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket Cap Size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket Cap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(categorize_market_cap)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Group by market cap size and sum up market cap values\u001b[39;00m\n\u001b[1;32m     26\u001b[0m grouped_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket Cap Size\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket Cap\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Market Cap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column\n",
    "data['Market Cap Size'] = data['Market Cap'].apply(categorize_market_cap)\n",
    "\n",
    "# Group by market cap size and sum up market cap values\n",
    "grouped_data = data.groupby('Market Cap Size').agg({'Market Cap': 'sum'})\n",
    "\n",
    "# Print the grouped data\n",
    "print(grouped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e76e3623-e443-4197-bbdd-68c713072b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 2023_MarketCap\n",
      "Market Cap Size                \n",
      "large-cap        22010460932096\n",
      "mega-cap         24008861351936\n",
      "micro-cap           54040462697\n",
      "mid-cap           3257538882816\n",
      "small-cap          643012293248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column\n",
    "data['Market Cap Size'] = data['2023_MarketCap'].apply(categorize_market_cap)\n",
    "\n",
    "# Group by market cap size and sum up market cap values\n",
    "grouped_data = data.groupby('Market Cap Size').agg({'2023_MarketCap': 'sum'})\n",
    "\n",
    "# Print the grouped data\n",
    "print(grouped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d768527-32ea-497c-897b-64c394e71107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Ticker  2023_MarketCap Market_Cap_Cat\n",
      "0         A     40807911424      large-cap\n",
      "1        AA      6412950016        mid-cap\n",
      "2       AAL      9062111232        mid-cap\n",
      "3      AAME        38368564      micro-cap\n",
      "4       AAN       214991008      micro-cap\n",
      "...     ...             ...            ...\n",
      "2578   USIO        39774540      micro-cap\n",
      "2579   VISL         9564498      micro-cap\n",
      "2580     WW       146528704      micro-cap\n",
      "2581   XFOR       189937888      micro-cap\n",
      "2582   ZYXI       363679296      small-cap\n",
      "\n",
      "[2583 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column and create a new column for the category\n",
    "data['Market_Cap_Cat'] = data['2023_MarketCap'].apply(categorize_market_cap)\n",
    "\n",
    "# Print the DataFrame with the new column\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44915dae-1eff-470d-954c-6de7a2448940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"ticker_2023_MarketCap.csv\")\n",
    "\n",
    "# Define the market cap size categories\n",
    "categories = {\n",
    "    'mega-cap': lambda x: x >= 200000000000,\n",
    "    'large-cap': lambda x: 10000000000 <= x < 200000000000,\n",
    "    'mid-cap': lambda x: 2000000000 <= x < 10000000000,\n",
    "    'small-cap': lambda x: 250000000 <= x < 2000000000,\n",
    "    'micro-cap': lambda x: x < 250000000\n",
    "}\n",
    "\n",
    "# Function to categorize market cap\n",
    "def categorize_market_cap(market_cap):\n",
    "    for category, condition in categories.items():\n",
    "        if condition(market_cap):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization to the market cap column and create a new column for the category\n",
    "data['Market_Cap_Cat'] = data['2023_MarketCap'].apply(categorize_market_cap)\n",
    "\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "data.to_csv(\"ticker_2023_MarketCap_with_Category.csv\", index=False)\n",
    "\n",
    "print(\"New CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e945162-6fee-40ef-8081-e6c784a57cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q6/3lfkn2n54rs43hmys3vv8k1w0000gn/T/ipykernel_40051/3443639948.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnet_income_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modified_net_income_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Merge the CSV files based on the 'Ticker' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_cap_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe_gsector_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_income_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save the merged data to a new CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stalwart_Master.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 148\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m         (\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Ticker'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "market_cap_data = pd.read_csv(\"ticker_2023_MarketCap_with_Category.csv\")\n",
    "pe_gsector_data = pd.read_csv(\"cleaned_PE_Gsector_with_avg.csv\")\n",
    "net_income_data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Merge the CSV files based on the 'Ticker' column\n",
    "merged_data = pd.merge(market_cap_data, pe_gsector_data, on='Ticker', how='outer')\n",
    "merged_data = pd.merge(merged_data, net_income_data, on='Ticker', how='outer')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Stalwart_Master.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e704595-363b-4ba6-bef4-d1caaa9a85fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "market_cap_data = pd.read_csv(\"ticker_2023_MarketCap_with_Category.csv\")\n",
    "pe_gsector_data = pd.read_csv(\"cleaned_PE_Gsector_with_avg.csv\")\n",
    "net_income_data = pd.read_csv(\"modified_net_income_data.csv\")\n",
    "\n",
    "# Rename the first column in net_income_data to \"Ticker\"\n",
    "net_income_data.rename(columns={net_income_data.columns[0]: 'Ticker'}, inplace=True)\n",
    "\n",
    "# Merge the CSV files based on the 'Ticker' column\n",
    "merged_data = pd.merge(market_cap_data, pe_gsector_data, on='Ticker', how='outer')\n",
    "merged_data = pd.merge(merged_data, net_income_data, on='Ticker', how='outer')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Stalwart_Master.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
